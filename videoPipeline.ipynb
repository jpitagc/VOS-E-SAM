{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gdown\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(sys.path[0]+\"/tracker\")\n",
    "sys.path.append(sys.path[0]+\"/tracker/model\")\n",
    "from track_anything import TrackingAnything\n",
    "from track_anything import parse_augment\n",
    "import requests\n",
    "import json\n",
    "import torchvision\n",
    "import torch \n",
    "from tools.painter import mask_painter\n",
    "import psutil\n",
    "import time\n",
    "try: \n",
    "    from mmcv.cnn import ConvModule\n",
    "except:\n",
    "    os.system(\"mim install mmcv\")\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools import mask as maskUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ovis_anotations = '../data.nosync/OVIS/annotations/'\n",
    "ovis_images = '../data.nosync/OVIS/train_images/'\n",
    "\n",
    "ovis_anotations = 'D:/HADA/data/OVIS/annotations/'\n",
    "ovis_images = 'D:/HADA/data/OVIS/train_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargarDatos(ruta_ann):\n",
    "    with open(ruta_ann + 'annotations_train.json') as f:\n",
    "        annotationsTrain = json.load(f)\n",
    "\n",
    "    with open(ruta_ann + 'annotations_valid.json') as f:\n",
    "        annotationsValid = json.load(f)\n",
    "\n",
    "    with open(ruta_ann + 'annotations_test.json') as f:\n",
    "        annotationsTest = json.load(f)\n",
    "\n",
    "    clases = annotationsTrain['categories']\n",
    "    vidTrain = annotationsTrain['videos']\n",
    "    annTrain = annotationsTrain['annotations']\n",
    "    vidValid = annotationsValid['videos']\n",
    "    annValid = annotationsValid['annotations']\n",
    "    vidTest = annotationsTest['videos']\n",
    "    annTest = annotationsTest['annotations']\n",
    "\n",
    "    return clases, vidTrain, annTrain, vidValid, annValid, vidTest, annTest\n",
    "\n",
    "clases, vidTrain, annTrain, vidValid, annValid, vidTest, annTest = cargarDatos(ovis_anotations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annToRLE(ann, frameId):\n",
    "    \"\"\"\n",
    "    Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "    :return: binary mask (numpy 2D array)\n",
    "    \"\"\"\n",
    "    h, w = ann['height'], ann['width']\n",
    "    segm = ann['segmentations'][frameId]\n",
    "    if segm is None:\n",
    "        return None\n",
    "    if type(segm) == \"list\":\n",
    "        # polygon -- a single object might consist of multiple parts\n",
    "        # we merge all parts into one mask rle code\n",
    "        rles = maskUtils.frPyObjects(segm, h, w)\n",
    "        rle = maskUtils.merge(rles)\n",
    "    elif type(segm['counts']) == \"list\":\n",
    "        # uncompressed RLE\n",
    "        rle = maskUtils.frPyObjects(segm, h, w)\n",
    "    else:\n",
    "        # rle\n",
    "        rle = segm\n",
    "    return rle\n",
    "\n",
    "\n",
    "def annToMask(ann, frameId):\n",
    "    \"\"\"\n",
    "    Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "    :return: binary mask (numpy 2D array)\n",
    "    \"\"\"\n",
    "    rle = annToRLE(ann, frameId)\n",
    "    if rle is not None:\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m\n",
    "\n",
    "\n",
    "\n",
    "def combineMasks(masks, width, height):\n",
    "    # Crear una matriz vacía para la máscara combinada\n",
    "    combined = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Combinar las máscaras en la matriz vacía\n",
    "    for mask in masks:\n",
    "        combined += mask  # Sumar la máscara a la máscara combinada\n",
    "\n",
    "    # Aplicar umbral para obtener una única máscara binaria\n",
    "    combined = np.where(combined > 0, 1, 0)\n",
    "    return combined\n",
    "\n",
    "def unifyMasks(masks, width, height):\n",
    "    # Crear una matriz vacía para la máscara combinada\n",
    "    unified = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Combinar las máscaras en la matriz vacía\n",
    "    for mask in masks:\n",
    "        unified += mask  # Sumar la máscara a la máscara combinada\n",
    "\n",
    "    \n",
    "    return unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(path,image_files):\n",
    "    images = []\n",
    "    for file in image_files:\n",
    "        img = cv2.imread(os.path.join(path,file))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "def load_all_initial_masks_from_dataset():\n",
    "    all_masks = []\n",
    "    for video in vidTrain:\n",
    "        ann = [a for a in annTrain if a['video_id'] == video['id']]\n",
    "        masks = [annToMask(a, 0) * (i + 1) for i, a in enumerate(ann) if annToMask(a, 0) is not None]\n",
    "        all_masks.append(unifyMasks(masks, video['width'], video['height']))\n",
    "    return all_masks\n",
    "\n",
    "def load_all_masks_for_video(video):\n",
    "    ann = [a for a in annTrain if a['video_id'] == video['id']]\n",
    "    all_masks  = []\n",
    "    for image_num in range(0,video['length']):\n",
    "        masks = []\n",
    "        for i, a in enumerate(ann):\n",
    "            annot = annToMask(a, image_num)\n",
    "            if annot is not None: masks.append(annot * (i + 1))\n",
    "        single_mask = unifyMasks(masks, video['width'], video['height'])\n",
    "        all_masks.append(single_mask)\n",
    "    return all_masks\n",
    "\n",
    "def generate_video_from_frames(frames, output_path, fps=30):\n",
    "    frames = torch.from_numpy(np.asarray(frames))\n",
    "    if not os.path.exists(os.path.dirname(output_path)):\n",
    "        os.makedirs(os.path.dirname(output_path))\n",
    "    torchvision.io.write_video(output_path, frames, fps=fps, video_codec=\"libx264\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(mask1, mask2):\n",
    "    # Ensure both masks have the same shape\n",
    "    assert mask1.shape == mask2.shape, \"Mask shapes must be the same.\"\n",
    "\n",
    "    # Calculate intersection and union for each label\n",
    "    labels = np.unique(np.concatenate((mask1, mask2)))\n",
    "    intersection = np.zeros_like(mask1, dtype=np.float32)\n",
    "    union = np.zeros_like(mask1, dtype=np.float32)\n",
    "    iou_per_label = {}\n",
    "\n",
    "    for label in labels:\n",
    "        mask1_label = mask1 == label\n",
    "        mask2_label = mask2 == label\n",
    "        c_intersection = np.logical_and(mask1_label, mask2_label)\n",
    "        c_union = np.logical_or(mask1_label, mask2_label)\n",
    "        intersection += c_intersection\n",
    "        union += c_union\n",
    "        iou_per_label[label] = np.sum(c_intersection) / np.sum(c_union)\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "    return iou, iou_per_label\n",
    "\n",
    "def compute_f_measure(mask1, mask2):\n",
    "    # Ensure both masks have the same shape\n",
    "    assert mask1.shape == mask2.shape, \"Mask shapes must be the same.\"\n",
    "\n",
    "    # Calculate F-measure for each label\n",
    "    labels = np.unique(np.concatenate((mask1, mask2)))\n",
    "    f_measure_per_label = {}\n",
    "\n",
    "    for label in labels:\n",
    "        mask1_label = mask1 == label\n",
    "        mask2_label = mask2 == label\n",
    "\n",
    "        true_positives = np.logical_and(mask1_label, mask2_label).sum()\n",
    "        false_positives = np.logical_and(mask1_label, np.logical_not(mask2_label)).sum()\n",
    "        false_negatives = np.logical_and(np.logical_not(mask1_label), mask2_label).sum()\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "        f_measure = 2 * (precision * recall) / (precision + recall)\n",
    "        f_measure_per_label[label] = f_measure\n",
    "\n",
    "    return f_measure_per_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_ovis_set(name, model,path_set,videos, annotations, compute_metrics = False,save_masks = False, compute_video = False, verbose = True):\n",
    "    for video in videos:\n",
    "        # Load all images as np.array\n",
    "        if verbose: print('Loading dataset images')\n",
    "        images = load_images_from_folder(path_set,video['file_names'])\n",
    "\n",
    "        # Load al poligon of first image to a usable mask\n",
    "        if verbose: print('Creating first annotated mask for VOS model')\n",
    "        ann = [a for a in annotations if a['video_id'] == video['id']]\n",
    "        masks = [(annToMask(a, 0) * (i + 1)) for i, a in enumerate(ann)]\n",
    "        initial_mask = unifyMasks(masks, video['width'], video['height'])\n",
    "\n",
    "        #Compute masks for all images\n",
    "        if verbose:print('Computing all masks')\n",
    "        model.xmem.clear_memory()\n",
    "        masks, logits, painted_images = model.generator(images=images, template_mask=initial_mask)\n",
    "        model.xmem.clear_memory()  \n",
    "\n",
    "        if compute_metrics:\n",
    "            if verbose: print('Computing Metrics')\n",
    "            ground_truth_masks = load_all_masks_for_video(video)\n",
    "            for i,(mask_infered, mask_gt) in enumerate(zip(masks[1:],ground_truth_masks[1:])):\n",
    "                f_measure = compute_f_measure(mask_infered,mask_gt)\n",
    "                iou, iou_per_label = calculate_iou(mask_infered,mask_gt)\n",
    "                print(f'Mask {i}: f_mesure{f_measure}, iou {iou}, per label {iou_per_label}')\n",
    "                \n",
    "        if compute_video: \n",
    "            if verbose: print('Generating video')\n",
    "            generate_video_from_frames(painted_images, output_path=\"./result/track/{}.mp4\".format('Video1'+name), fps = 30) \n",
    "\n",
    "        if save_masks:\n",
    "            if verbose: print('Saving masks') \n",
    "            path_to_masks = './result/mask/{}'.format('Video1'+name)\n",
    "            if not os.path.exists(path_to_masks): os.makedirs(path_to_masks)\n",
    "            for i,mask in enumerate(masks): np.save(os.path.join(path_to_masks, '{:05d}.npy'.format(i)), mask)\n",
    "                \n",
    "    return masks, logits, painted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BaseSegmenter to cuda:0\n",
      "Hyperparameters read from the model weights: C^k=64, C^v=512, C^h=64\n",
      "Single object mode: False\n",
      "Sam Refinement ACTIVATED. Mode: both\n",
      "Initializing BaseSegmenter to cuda:0\n",
      "Hyperparameters read from the model weights: C^k=64, C^v=512, C^h=64\n",
      "Single object mode: False\n",
      "Sam Refinement ACTIVATED. Mode: both_neg\n"
     ]
    }
   ],
   "source": [
    "SAM_checkpoint = \"./checkpoints/sam_vit_h_4b8939.pth\"\n",
    "xmem_checkpoint = \"./checkpoints/XMem-s012.pth\"\n",
    "e2fgvi_checkpoint = \"./checkpoints/E2FGVI-HQ-CVPR22.pth\"\n",
    "''' args = {\n",
    "    'use_refinement' : False\n",
    "        }\n",
    "model = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)\n",
    "\n",
    "\n",
    "args = {\n",
    "    'use_refinement' : True,\n",
    "    'refinement_mode' : 'bbox'\n",
    "         }\n",
    "modelSamBbox = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)\n",
    "\n",
    "\n",
    "args = {\n",
    "   'use_refinement' : True,\n",
    "   'refinement_mode' : 'point'\n",
    "       }\n",
    "modelSamPoint = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)\n",
    "\n",
    "'''\n",
    "\n",
    "args = {\n",
    "   'use_refinement' : True,\n",
    "   'refinement_mode' : 'both'\n",
    "       }\n",
    "modelBoth = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)\n",
    "\n",
    "args = {\n",
    "   'use_refinement' : True,\n",
    "   'refinement_mode' : 'both_neg'\n",
    "       }\n",
    "modelBothNeg = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset images\n",
      "Creating first annotated mask for VOS model\n",
      "Computing all masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:   1%|          | 1/175 [00:03<09:28,  3.27s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BaseTracker.get_very_very_best_point_of_interest() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m masks_refinement_bbox, logits_refinement_bbox, painted_images_refinement_bbox \u001b[39m=\u001b[39m run_model_on_ovis_set(name \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m_both_neg_HQTA_lizard\u001b[39;49m\u001b[39m'\u001b[39;49m,model \u001b[39m=\u001b[39;49m modelBothNeg, path_set \u001b[39m=\u001b[39;49m ovis_images,videos \u001b[39m=\u001b[39;49m vidTrain[\u001b[39m37\u001b[39;49m:\u001b[39m38\u001b[39;49m],annotations \u001b[39m=\u001b[39;49m annTrain,compute_metrics \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, save_masks\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, compute_video\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m, in \u001b[0;36mrun_model_on_ovis_set\u001b[1;34m(name, model, path_set, videos, annotations, compute_metrics, save_masks, compute_video, verbose)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mif\u001b[39;00m verbose:\u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mComputing all masks\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m model\u001b[39m.\u001b[39mxmem\u001b[39m.\u001b[39mclear_memory()\n\u001b[1;32m---> 16\u001b[0m masks, logits, painted_images \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerator(images\u001b[39m=\u001b[39;49mimages, template_mask\u001b[39m=\u001b[39;49minitial_mask)\n\u001b[0;32m     17\u001b[0m model\u001b[39m.\u001b[39mxmem\u001b[39m.\u001b[39mclear_memory()  \n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m compute_metrics:\n",
      "File \u001b[1;32mc:\\Users\\GIAA\\Documents\\Pita\\code\\Track-Anything\\track_anything.py:59\u001b[0m, in \u001b[0;36mTrackingAnything.generator\u001b[1;34m(self, images, template_mask)\u001b[0m\n\u001b[0;32m     56\u001b[0m     painted_images\u001b[39m.\u001b[39mappend(painted_image)\n\u001b[0;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     mask, logit, painted_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxmem\u001b[39m.\u001b[39;49mtrack(images[i])\n\u001b[0;32m     60\u001b[0m     masks\u001b[39m.\u001b[39mappend(mask)\n\u001b[0;32m     61\u001b[0m     logits\u001b[39m.\u001b[39mappend(logit)\n",
      "File \u001b[1;32mc:\\Users\\GIAA\\Documents\\Pita\\code\\Track-Anything\\.venvTA\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GIAA\\Documents\\Pita\\code\\Track-Anything\\tracker\\base_tracker.py:122\u001b[0m, in \u001b[0;36mBaseTracker.track\u001b[1;34m(self, frame, first_frame_annotation)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m###########\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39m######3####\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m first_frame_annotation \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msam_model:\n\u001b[0;32m    121\u001b[0m     \u001b[39m#print('Sam Refinment. Mode: ' + self.sam_refinement_mode)\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     out_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_sam_refinement(frame,out_mask)\n\u001b[0;32m    124\u001b[0m final_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(out_mask)\n\u001b[0;32m    126\u001b[0m \u001b[39m# map back\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\GIAA\\Documents\\Pita\\code\\Track-Anything\\.venvTA\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GIAA\\Documents\\Pita\\code\\Track-Anything\\tracker\\base_tracker.py:350\u001b[0m, in \u001b[0;36mBaseTracker.custom_sam_refinement\u001b[1;34m(self, frame, out_mask)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msam_refinement_mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mboth_neg\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    349\u001b[0m     bounding_boxes \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_bounding_box(mask) \u001b[39mfor\u001b[39;00m mask \u001b[39min\u001b[39;00m all_masks_separated]\n\u001b[1;32m--> 350\u001b[0m     points_of_interest \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_very_very_best_point_of_interest(mask,\u001b[39m5\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m mask \u001b[39min\u001b[39;49;00m all_masks_separated]\n\u001b[0;32m    351\u001b[0m     negative_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfind_neg_points(bounding_boxes,points_of_interest)\n\u001b[0;32m    352\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_image_bbox(out_mask,bounding_boxes,points_of_interest)\n",
      "File \u001b[1;32mc:\\Users\\GIAA\\Documents\\Pita\\code\\Track-Anything\\tracker\\base_tracker.py:350\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msam_refinement_mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mboth_neg\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    349\u001b[0m     bounding_boxes \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_bounding_box(mask) \u001b[39mfor\u001b[39;00m mask \u001b[39min\u001b[39;00m all_masks_separated]\n\u001b[1;32m--> 350\u001b[0m     points_of_interest \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_very_very_best_point_of_interest(mask,\u001b[39m5\u001b[39;49m) \u001b[39mfor\u001b[39;00m mask \u001b[39min\u001b[39;00m all_masks_separated]\n\u001b[0;32m    351\u001b[0m     negative_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfind_neg_points(bounding_boxes,points_of_interest)\n\u001b[0;32m    352\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_image_bbox(out_mask,bounding_boxes,points_of_interest)\n",
      "\u001b[1;31mTypeError\u001b[0m: BaseTracker.get_very_very_best_point_of_interest() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "masks_refinement_bbox, logits_refinement_bbox, painted_images_refinement_bbox = run_model_on_ovis_set(name = '_both_neg_HQTA_lizard',model = modelBothNeg, path_set = ovis_images,videos = vidTrain[37:38],annotations = annTrain,compute_metrics = False, save_masks=False, compute_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset images\n",
      "Creating first annotated mask for VOS model\n",
      "Computing all masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image: 100%|██████████| 175/175 [01:07<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating video\n"
     ]
    }
   ],
   "source": [
    "masks_refinement_bbox, logits_refinement_bbox, painted_images_refinement_bbox = run_model_on_ovis_set(name = '_both_HQTA_lizard',model = modelBoth, path_set = ovis_images,videos = vidTrain[37:38],annotations = annTrain,compute_metrics = False, save_masks=False, compute_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, logits, painted_images = run_model_on_ovis_set(name = '_no_ref_lizards',model = model, path_set = ovis_images,videos = vidTrain[37:38],annotations = annTrain,compute_metrics = False, save_masks=False, compute_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_refinement_point, logits_refinement_point, painted_images_refinement_point = run_model_on_ovis_set(name = '_point_ref_lizard',model = modelSamPoint, path_set = ovis_images,videos = vidTrain[37:38],annotations = annTrain,compute_metrics = False, save_masks=False, compute_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_images(image1, image2, image3):\n",
    "    fig = plt.figure(figsize=(30, 15))\n",
    "\n",
    "    # Create a grid with two subplots\n",
    "    grid = plt.GridSpec(1, 3)\n",
    "\n",
    "    # Display the first image in the left subplot\n",
    "    ax1 = plt.subplot(grid[0])\n",
    "    ax1.imshow(image1)\n",
    "    ax1.set_title('Image')\n",
    "\n",
    "    # Display the first image in the left subplot\n",
    "    ax1 = plt.subplot(grid[1])\n",
    "    ax1.imshow(image2)\n",
    "    ax1.set_title('No Ref')\n",
    "\n",
    "    # Display the second image in the right subplot\n",
    "    ax2 = plt.subplot(grid[2])\n",
    "    ax2.imshow(image3)\n",
    "    ax2.set_title('Ref')\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.subplots_adjust(wspace=0.1)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images_from_folder(ovis_images,vidTrain[37]['file_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(153,157):\n",
    "    plt.imshow(images[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in range(0,10):\n",
    " print_images(images[id],painted_images[id],painted_images_refinement_bbox[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in painted_images_refinement_point: \n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks_refinement_point[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in painted_images: \n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUNK TESTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images_from_folder(ovis_images,vidTrain[0]['file_names'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_point_of_interest(segmentation_mask):\n",
    "    # Find contours in the segmentation mask\n",
    "    points = []\n",
    "    contours, _ = cv2.findContours(segmentation_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for contour in contours:\n",
    "        # Extract the bounding box coordinates of the contour\n",
    "        M = cv2.moments(contour)\n",
    "        points.append([int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"])])\n",
    "\n",
    "    return np.array(points).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = [a for a in annTrain if a['video_id'] == vidTrain[0]['id']]\n",
    "masks = [(annToMask(a, 0) * (i + 1)) for i, a in enumerate(ann)]\n",
    "initial_mask = unifyMasks(masks, vidTrain[0]['width'], vidTrain[0]['height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_point_of_interest(masks[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = []\n",
    "for mask in masks:\n",
    "    \n",
    "    points = get_best_point_of_interest(mask)\n",
    "    all_points.append(points)\n",
    "    plt.imshow(mask)\n",
    "    print(points[0][0])\n",
    "    print(all_points)\n",
    "    plt.scatter(points[0][0], points[0][1], c='red', marker='o')\n",
    "\n",
    "    # Set the axis limits\n",
    "    plt.xlim(0, mask.shape[1])\n",
    "    plt.ylim(mask.shape[0], 0)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = np.array(all_points).astype('uint8')\n",
    "all_labels = np.ones((all_points.shape[0],1)).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.reset_image()\n",
    "modelSam.xmem.sam_model.sam_controler.set_image(images[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.predictor.predict(point_coords=all_points[0], \n",
    "                                point_labels=all_labels[0], \n",
    "                                multimask_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'point'\n",
    "prompts = {\n",
    "    'point_coords': all_points[0],\n",
    "    'point_labels': all_labels[0], \n",
    "}\n",
    "modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.predictor.predict(point_coords=all_points, \n",
    "                                point_labels=all_labels, \n",
    "                                multimask_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM_checkpoint = \"./checkpoints/sam_vit_h_4b8939.pth\"\n",
    "xmem_checkpoint = \"./checkpoints/XMem-s012.pth\"\n",
    "e2fgvi_checkpoint = \"./checkpoints/E2FGVI-HQ-CVPR22.pth\"\n",
    "args = {'use_refinement':False}\n",
    "model = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)\n",
    "args = {'use_refinement':True}\n",
    "modelSam = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images_from_folder(ovis_images,vidTrain[0]['file_names'])\n",
    "#model.xmem.sam_refinement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = [a for a in annTrain if a['video_id'] == vidTrain[0]['id']]\n",
    "masks = [(annToMask(a, 0) * (i + 1)) for i, a in enumerate(ann)]\n",
    "initial_mask = unifyMasks(masks, vidTrain[0]['width'], vidTrain[0]['height'])\n",
    "masks = [(annToMask(a, 0)) for i, a in enumerate(ann)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resized_mask = cv2.resize(masks[3], (256, 256), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(masks[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bounding_box(segmentation_mask):\n",
    "    # Get the indices where the segmentation mask is non-zero\n",
    "    nonzero_indices = np.nonzero(segmentation_mask)\n",
    "    \n",
    "    # Calculate the bounding box coordinates\n",
    "    min_row = np.min(nonzero_indices[0])\n",
    "    max_row = np.max(nonzero_indices[0])\n",
    "    min_col = np.min(nonzero_indices[1])\n",
    "    max_col = np.max(nonzero_indices[1])\n",
    "    \n",
    "    # Return the bounding box coordinates as a tuple\n",
    "    bounding_box = [min_col,min_row, max_col, max_row]\n",
    "    return bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = [compute_bounding_box(mask) for mask in masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = torch.tensor(bboxes, device=modelSam.xmem.sam_model.sam_controler.predictor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_boxes = modelSam.xmem.sam_model.sam_controler.predictor.transform.apply_boxes_torch(input_boxes, images[0].shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.reset_image()\n",
    "modelSam.xmem.sam_model.sam_controler.set_image(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0])\n",
    "plt.show()\n",
    "# mask only ------------------------\n",
    "mode = 'bounding_boxes'\n",
    "prompts = {'bounding_boxes': transformed_boxes}\n",
    "\n",
    "masksout, scores, logits = modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=False)  # masks (n, h, w), scores (n,), logits (n, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masksout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_mask = np.zeros_like(masksout[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(pp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masksout[0] * 1 + masksout[1] * 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_mask[masksout[1]] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masksout[0] == pp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(scores)):\n",
    "    if i == np.argmax(scores): print('Selected')\n",
    "    painted_image = mask_painter(images[0], masksout[i][0].numpy().astype('uint8'))\n",
    "    show_box(input_boxes[i], plt.gca())\n",
    "    plt.imshow(painted_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(images[0])\n",
    "plt.show()\n",
    "# mask only ------------------------\n",
    "mode = 'bbox'\n",
    "prompts = {'bounding_box': transformed_boxes}\n",
    "\n",
    "masksout, scores, logits = modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=False)  # masks (n, h, w), scores (n,), logits (n, 256, 256)\n",
    "for i in range(0,len(scores)):\n",
    "    if i == np.argmax(scores): print('Selected')\n",
    "    painted_image = mask_painter(images[1], masksout[i].astype('uint8'))\n",
    "    show_box(bb, plt.gca())\n",
    "    plt.imshow(painted_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.predictor.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelSam.xmem.sam_model.sam_controler.reset_image()\n",
    "#modelSam.xmem.sam_model.sam_controler.set_image(images[0])\n",
    "plt.imshow(images[0])\n",
    "\n",
    "# mask only ------------------------\n",
    "mode = 'mask'\n",
    "prompts = {'mask_input': masks[3][None,:,:]}\n",
    "\n",
    "masks, scores, logits = modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=True)  # masks (n, h, w), scores (n,), logits (n, 256, 256)\n",
    "for i in range(0,len(scores)):\n",
    "    if i == np.argmax(scores): print('Selected')\n",
    "    painted_image = mask_painter(images[1], masks[i].astype('uint8'))\n",
    "    plt.imshow(painted_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.xmem.clear_memory()\n",
    "masksout, logitsout, painted_imagesout = model.generator(images=images[0:2], template_mask=initial_mask)\n",
    "model.xmem.clear_memory() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "resizer = Resize([256, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triallogits = logitsout[1][0].unsqueeze(0)\n",
    "ind_logits = resizer(triallogits).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painted_imagesout[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.reset_image()\n",
    "modelSam.xmem.sam_model.sam_controler.set_image(images[0])\n",
    "plt.imshow(images[0])\n",
    "\n",
    "# mask only ------------------------\n",
    "mode = 'mask'\n",
    "prompts = {'mask_input': ind_logits}\n",
    "\n",
    "masks, scores, logits = modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=True)  # masks (n, h, w), scores (n,), logits (n, 256, 256)\n",
    "for i in range(0,len(scores)):\n",
    "    if i == np.argmax(scores): print('Selected')\n",
    "    painted_image = mask_painter(images[1], masks[i].astype('uint8'))\n",
    "    plt.imshow(painted_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.reset_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelSam.xmem.sam_model.sam_controler.set_image(images[0])\n",
    "#model.samcontroler.sam_controler.set_image(images[0])\n",
    "mode = 'point'\n",
    "prompts = {\n",
    "    'point_coords': np.array([[500, 650]]),\n",
    "    'point_labels': np.array([1]), \n",
    "}\n",
    "masks, scores, logits = modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=False)  # masks (n, h, w), scores (n,), logits (n, 256, 256)\n",
    "painted_image = mask_painter(images[0], masks[np.argmax(scores)].astype('uint8'))\n",
    "#cv2.imwrite('/hhd3/gaoshang/truck_point.jpg', painted_image)\n",
    "\n",
    "\n",
    "plt.imshow(painted_image)\n",
    "plt.show()\n",
    "\n",
    "# mask only ------------------------\n",
    "mode = 'mask'\n",
    "mask_input  = logits[np.argmax(scores), :, :]\n",
    "\n",
    "prompts = {'mask_input': mask_input[None, :, :]}\n",
    "print(prompts['mask_input'].shape)\n",
    "\n",
    "masks, scores, logits = modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=True)  # masks (n, h, w), scores (n,), logits (n, 256, 256)\n",
    "for i in range(0,len(scores)):\n",
    "    if i == np.argmax(scores): print('Selected')\n",
    "    painted_image = mask_painter(images[0], masks[i].astype('uint8'))\n",
    "    plt.imshow(painted_image)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee una imagen y la muestra en una ventanaovis_images + first_video_folder +'/img_0000001.jpg'\n",
    "img = cv2.imread(os.path.join(ovis_images + first_video_folder +'/img_0000001.jpg'))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    (255, 0, 0),    # Rojo\n",
    "    (0, 255, 0),    # Verde\n",
    "    (0, 0, 255),    # Azul\n",
    "    (255, 255, 0),  # Amarillo\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (0, 255, 255),  # Cian\n",
    "    (128, 0, 0),    # Marrón oscuro\n",
    "    (0, 128, 0),    # Verde oscuro\n",
    "    (0, 0, 128),    # Azul oscuro\n",
    "    (128, 128, 0),  # Amarillo oscuro\n",
    "    (128, 0, 128),  # Magenta oscuro\n",
    "    (0, 128, 128),  # Cian oscuro\n",
    "    (255, 128, 0),  # Naranja\n",
    "    (128, 255, 0),  # Lima\n",
    "    (255, 0, 128),  # Rosa\n",
    "    (128, 0, 255),  # Violeta\n",
    "    (0, 255, 128),  # Turquesa\n",
    "    (0, 128, 255),  # Azul claro\n",
    "    (255, 128, 128), # Rosa claro\n",
    "    (128, 255, 128)  # Verde claro\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = []\n",
    "video = vidTrain[0]\n",
    "for a in annTrain:\n",
    "    if a['video_id'] == video['id']:\n",
    "        ann.append(a)\n",
    "        break\n",
    "    else: continue\n",
    "\n",
    "mask = annToMask(ann[0], 0)\n",
    "colored_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "colored_mask[:, :, 0] = mask * colors[0][0]\n",
    "colored_mask[:, :, 1] = mask * colors[0][1]\n",
    "colored_mask[:, :, 2] = mask * colors[0][2]\n",
    "plt.imshow(colored_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = []\n",
    "for a in annTrain:\n",
    "    if a['video_id'] == video['id']:\n",
    "        ann.append(a)\n",
    "    else: continue\n",
    "\n",
    "masks = []\n",
    "for i,a in enumerate(ann):\n",
    "    m = annToMask(a, 0)\n",
    "    m = m * (i + 1)\n",
    "    masks.append(m)\n",
    "\n",
    "w, h = video['width'], video['height']\n",
    "unified = unifyMasks(masks, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.xmem.clear_memory()\n",
    "masks, logits, painted_images = model.generator(images=images[0:5], template_mask=unified)\n",
    "model.xmem.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(painted_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rgb_mask(mask):\n",
    "    colored_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    colored_mask[:, :, 0] = mask * colors[0][0]\n",
    "    colored_mask[:, :, 1] = mask * colors[0][1]\n",
    "    colored_mask[:, :, 2] = mask * colors[0][2]\n",
    "    plt.imshow(colored_mask)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rgb_mask(logits[29])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
