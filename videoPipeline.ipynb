{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gdown\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(sys.path[0]+\"/tracker\")\n",
    "sys.path.append(sys.path[0]+\"/tracker/model\")\n",
    "from track_anything import TrackingAnything\n",
    "from track_anything import parse_augment\n",
    "import requests\n",
    "import json\n",
    "import torchvision\n",
    "import torch \n",
    "from tools.painter import mask_painter\n",
    "import psutil\n",
    "import time\n",
    "try: \n",
    "    from mmcv.cnn import ConvModule\n",
    "except:\n",
    "    os.system(\"mim install mmcv\")\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools import mask as maskUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ovis_anotations = '../data.nosync/OVIS/annotations/'\n",
    "ovis_images = '../data.nosync/OVIS/train_images/'\n",
    "\n",
    "ovis_anotations = 'D:/HADA/data/OVIS/annotations/'\n",
    "ovis_images = 'D:/HADA/data/OVIS/train_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargarDatos(ruta_ann):\n",
    "    with open(ruta_ann + 'annotations_train.json') as f:\n",
    "        annotationsTrain = json.load(f)\n",
    "\n",
    "    with open(ruta_ann + 'annotations_valid.json') as f:\n",
    "        annotationsValid = json.load(f)\n",
    "\n",
    "    with open(ruta_ann + 'annotations_test.json') as f:\n",
    "        annotationsTest = json.load(f)\n",
    "\n",
    "    clases = annotationsTrain['categories']\n",
    "    vidTrain = annotationsTrain['videos']\n",
    "    annTrain = annotationsTrain['annotations']\n",
    "    vidValid = annotationsValid['videos']\n",
    "    annValid = annotationsValid['annotations']\n",
    "    vidTest = annotationsTest['videos']\n",
    "    annTest = annotationsTest['annotations']\n",
    "\n",
    "    return clases, vidTrain, annTrain, vidValid, annValid, vidTest, annTest\n",
    "\n",
    "clases, vidTrain, annTrain, vidValid, annValid, vidTest, annTest = cargarDatos(ovis_anotations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annToRLE(ann, frameId):\n",
    "    \"\"\"\n",
    "    Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "    :return: binary mask (numpy 2D array)\n",
    "    \"\"\"\n",
    "    h, w = ann['height'], ann['width']\n",
    "    segm = ann['segmentations'][frameId]\n",
    "    if segm is None:\n",
    "        return None\n",
    "    if type(segm) == \"list\":\n",
    "        # polygon -- a single object might consist of multiple parts\n",
    "        # we merge all parts into one mask rle code\n",
    "        rles = maskUtils.frPyObjects(segm, h, w)\n",
    "        rle = maskUtils.merge(rles)\n",
    "    elif type(segm['counts']) == \"list\":\n",
    "        # uncompressed RLE\n",
    "        rle = maskUtils.frPyObjects(segm, h, w)\n",
    "    else:\n",
    "        # rle\n",
    "        rle = segm\n",
    "    return rle\n",
    "\n",
    "\n",
    "def annToMask(ann, frameId):\n",
    "    \"\"\"\n",
    "    Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "    :return: binary mask (numpy 2D array)\n",
    "    \"\"\"\n",
    "    rle = annToRLE(ann, frameId)\n",
    "    if rle is not None:\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m\n",
    "\n",
    "\n",
    "\n",
    "def combineMasks(masks, width, height):\n",
    "    # Crear una matriz vacía para la máscara combinada\n",
    "    combined = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Combinar las máscaras en la matriz vacía\n",
    "    for mask in masks:\n",
    "        combined += mask  # Sumar la máscara a la máscara combinada\n",
    "\n",
    "    # Aplicar umbral para obtener una única máscara binaria\n",
    "    combined = np.where(combined > 0, 1, 0)\n",
    "    return combined\n",
    "\n",
    "def unifyMasks(masks, width, height):\n",
    "    # Crear una matriz vacía para la máscara combinada\n",
    "    unified = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Combinar las máscaras en la matriz vacía\n",
    "    for mask in masks:\n",
    "        unified += mask  # Sumar la máscara a la máscara combinada\n",
    "\n",
    "    \n",
    "    return unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(path,image_files):\n",
    "    images = []\n",
    "    for file in image_files:\n",
    "        img = cv2.imread(os.path.join(path,file))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "def load_all_initial_masks_from_dataset():\n",
    "    all_masks = []\n",
    "    for video in vidTrain:\n",
    "        ann = [a for a in annTrain if a['video_id'] == video['id']]\n",
    "        masks = [annToMask(a, 0) * (i + 1) for i, a in enumerate(ann) if annToMask(a, 0) is not None]\n",
    "        all_masks.append(unifyMasks(masks, video['width'], video['height']))\n",
    "    return all_masks\n",
    "\n",
    "def load_all_masks_for_video(video):\n",
    "    ann = [a for a in annTrain if a['video_id'] == video['id']]\n",
    "    all_masks  = []\n",
    "    for image_num in range(0,video['length']):\n",
    "        masks = []\n",
    "        for i, a in enumerate(ann):\n",
    "            annot = annToMask(a, image_num)\n",
    "            if annot is not None: masks.append(annot * (i + 1))\n",
    "        single_mask = unifyMasks(masks, video['width'], video['height'])\n",
    "        all_masks.append(single_mask)\n",
    "    return all_masks\n",
    "\n",
    "def generate_video_from_frames(frames, output_path, fps=30):\n",
    "    frames = torch.from_numpy(np.asarray(frames))\n",
    "    if not os.path.exists(os.path.dirname(output_path)):\n",
    "        os.makedirs(os.path.dirname(output_path))\n",
    "    torchvision.io.write_video(output_path, frames, fps=fps, video_codec=\"libx264\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(mask1, mask2):\n",
    "    # Ensure both masks have the same shape\n",
    "    assert mask1.shape == mask2.shape, \"Mask shapes must be the same.\"\n",
    "\n",
    "    # Calculate intersection and union for each label\n",
    "    labels = np.unique(np.concatenate((mask1, mask2)))\n",
    "    intersection = np.zeros_like(mask1, dtype=np.float32)\n",
    "    union = np.zeros_like(mask1, dtype=np.float32)\n",
    "    iou_per_label = {}\n",
    "\n",
    "    for label in labels:\n",
    "        mask1_label = mask1 == label\n",
    "        mask2_label = mask2 == label\n",
    "        c_intersection = np.logical_and(mask1_label, mask2_label)\n",
    "        c_union = np.logical_or(mask1_label, mask2_label)\n",
    "        intersection += c_intersection\n",
    "        union += c_union\n",
    "        iou_per_label[label] = np.sum(c_intersection) / np.sum(c_union)\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "    return iou, iou_per_label\n",
    "\n",
    "def compute_f_measure(mask1, mask2):\n",
    "    # Ensure both masks have the same shape\n",
    "    assert mask1.shape == mask2.shape, \"Mask shapes must be the same.\"\n",
    "\n",
    "    # Calculate F-measure for each label\n",
    "    labels = np.unique(np.concatenate((mask1, mask2)))\n",
    "    f_measure_per_label = {}\n",
    "\n",
    "    for label in labels:\n",
    "        mask1_label = mask1 == label\n",
    "        mask2_label = mask2 == label\n",
    "\n",
    "        true_positives = np.logical_and(mask1_label, mask2_label).sum()\n",
    "        false_positives = np.logical_and(mask1_label, np.logical_not(mask2_label)).sum()\n",
    "        false_negatives = np.logical_and(np.logical_not(mask1_label), mask2_label).sum()\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "        f_measure = 2 * (precision * recall) / (precision + recall)\n",
    "        f_measure_per_label[label] = f_measure\n",
    "\n",
    "    return f_measure_per_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_ovis_set(name, model,path_set,videos, annotations, compute_metrics = False,save_masks = False, compute_video = False, verbose = True):\n",
    "    for video in videos:\n",
    "        # Load all images as np.array\n",
    "        if verbose: print('Loading dataset images')\n",
    "        images = load_images_from_folder(path_set,video['file_names'])\n",
    "\n",
    "        # Load al poligon of first image to a usable mask\n",
    "        if verbose: print('Creating first annotated mask for VOS model')\n",
    "        ann = [a for a in annotations if a['video_id'] == video['id']]\n",
    "        masks = [(annToMask(a, 0) * (i + 1)) for i, a in enumerate(ann)]\n",
    "        initial_mask = unifyMasks(masks, video['width'], video['height'])\n",
    "\n",
    "        #Compute masks for all images\n",
    "        if verbose:print('Computing all masks')\n",
    "        model.xmem.clear_memory()\n",
    "        masks, logits, painted_images = model.generator(images=images, template_mask=initial_mask)\n",
    "        model.xmem.clear_memory()  \n",
    "\n",
    "        if compute_metrics:\n",
    "            if verbose: print('Computing Metrics')\n",
    "            ground_truth_masks = load_all_masks_for_video(video)\n",
    "            for i,(mask_infered, mask_gt) in enumerate(zip(masks[1:],ground_truth_masks[1:])):\n",
    "                f_measure = compute_f_measure(mask_infered,mask_gt)\n",
    "                iou, iou_per_label = calculate_iou(mask_infered,mask_gt)\n",
    "                print(f'Mask {i}: f_mesure{f_measure}, iou {iou}, per label {iou_per_label}')\n",
    "                \n",
    "        if compute_video: \n",
    "            if verbose: print('Generating video')\n",
    "            generate_video_from_frames(painted_images, output_path=\"./result/track/{}.mp4\".format('Video1'+name), fps = 30) \n",
    "\n",
    "        if save_masks:\n",
    "            if verbose: print('Saving masks') \n",
    "            path_to_masks = './result/mask/{}'.format('Video1'+name)\n",
    "            if not os.path.exists(path_to_masks): os.makedirs(path_to_masks)\n",
    "            for i,mask in enumerate(masks): np.save(os.path.join(path_to_masks, '{:05d}.npy'.format(i)), mask)\n",
    "                \n",
    "    return masks, logits, painted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BaseSegmenter to cuda:0\n",
      "Hyperparameters read from the model weights: C^k=64, C^v=512, C^h=64\n",
      "Single object mode: False\n",
      "Sam Refinement ACTIVATED. Mode: mask_bbox_pos_neg\n"
     ]
    }
   ],
   "source": [
    "SAM_checkpoint = \"./checkpoints/sam_vit_h_4b8939.pth\"\n",
    "xmem_checkpoint = \"./checkpoints/XMem-s012.pth\"\n",
    "e2fgvi_checkpoint = \"./checkpoints/E2FGVI-HQ-CVPR22.pth\"\n",
    "''' args = {\n",
    "    'use_refinement' : False\n",
    "        }\n",
    "model = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)\n",
    "\n",
    "\n",
    "args = {\n",
    "    'use_refinement' : True,\n",
    "    'refinement_mode' : 'bbox'\n",
    "         }\n",
    "modelSamBbox = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)\n",
    "\n",
    "\n",
    "args = {\n",
    "   'use_refinement' : True,\n",
    "   'refinement_mode' : 'point'\n",
    "       }\n",
    "modelSamPoint = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)\n",
    "\n",
    "\n",
    "\n",
    "args = {\n",
    "   'use_refinement' : True,\n",
    "   'refinement_mode' : 'both'\n",
    "       }\n",
    "modelBoth = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)\n",
    "\n",
    "'''\n",
    "args = {\n",
    "   'use_refinement' : True,\n",
    "   'refinement_mode' : 'mask_bbox_pos_neg'\n",
    "       }\n",
    "modelBothNeg = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset images\n",
      "Creating first annotated mask for VOS model\n",
      "Computing all masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:   1%|          | 1/175 [00:02<07:34,  2.61s/it]c:\\Users\\GIAA\\Documents\\Pita\\code\\Track-Anything\\.venvTA\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Tracking image:   2%|▏         | 3/175 [00:03<02:40,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 507,414\n",
      "To: 503,418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:   6%|▋         | 11/175 [00:06<00:59,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 486,421\n",
      "To: 487,431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:   8%|▊         | 14/175 [00:07<00:57,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 469,437\n",
      "To: 464,436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  10%|▉         | 17/175 [00:08<00:55,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 158,357\n",
      "To: 156,357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  14%|█▎        | 24/175 [00:10<00:52,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 168,363\n",
      "To: 167,366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  14%|█▍        | 25/175 [00:11<00:51,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 168,364\n",
      "To: 168,366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  15%|█▍        | 26/175 [00:11<00:51,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 167,363\n",
      "To: 165,364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  15%|█▌        | 27/175 [00:11<00:51,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 166,363\n",
      "To: 166,365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  16%|█▌        | 28/175 [00:12<00:50,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 167,366\n",
      "To: 166,367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  17%|█▋        | 29/175 [00:12<00:50,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 166,368\n",
      "To: 164,368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  17%|█▋        | 30/175 [00:12<00:50,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 165,370\n",
      "To: 164,370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  21%|██        | 36/175 [00:14<00:48,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 394,23\n",
      "To: 392,24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  23%|██▎       | 40/175 [00:16<00:46,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 303,317\n",
      "To: 306,321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  23%|██▎       | 41/175 [00:16<00:46,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 535,287\n",
      "To: 535,290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  24%|██▍       | 42/175 [00:17<00:46,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 303,330\n",
      "To: 306,334\n",
      "Correcting Point: 538,284\n",
      "To: 539,285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  25%|██▍       | 43/175 [00:17<00:46,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 296,328\n",
      "To: 304,335\n",
      "Correcting Point: 535,282\n",
      "To: 535,284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  25%|██▌       | 44/175 [00:17<00:46,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 301,321\n",
      "To: 310,331\n",
      "Correcting Point: 533,280\n",
      "To: 533,282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  26%|██▌       | 45/175 [00:18<00:45,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 302,321\n",
      "To: 310,334\n",
      "Correcting Point: 176,385\n",
      "To: 176,386\n",
      "Correcting Point: 532,276\n",
      "To: 533,278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  26%|██▋       | 46/175 [00:18<00:45,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 151,299\n",
      "To: 147,298\n",
      "Correcting Point: 528,275\n",
      "To: 529,279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  27%|██▋       | 47/175 [00:18<00:45,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 290,338\n",
      "To: 301,357\n",
      "Correcting Point: 525,282\n",
      "To: 528,291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  27%|██▋       | 48/175 [00:19<00:44,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 299,338\n",
      "To: 310,356\n",
      "Correcting Point: 530,286\n",
      "To: 532,292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  28%|██▊       | 49/175 [00:19<00:44,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 292,338\n",
      "To: 304,361\n",
      "Correcting Point: 178,389\n",
      "To: 176,389\n",
      "Correcting Point: 520,283\n",
      "To: 525,294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  29%|██▊       | 50/175 [00:19<00:43,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 304,337\n",
      "To: 314,358\n",
      "Correcting Point: 520,283\n",
      "To: 524,294\n",
      "Correcting Point: 384,274\n",
      "To: 384,273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  29%|██▉       | 51/175 [00:20<00:43,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 304,337\n",
      "To: 311,357\n",
      "Correcting Point: 520,283\n",
      "To: 523,294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  30%|██▉       | 52/175 [00:20<00:43,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 241,361\n",
      "To: 242,375\n",
      "Correcting Point: 524,281\n",
      "To: 528,292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  30%|███       | 53/175 [00:20<00:42,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 246,360\n",
      "To: 249,373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  31%|███       | 54/175 [00:21<00:42,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 348,283\n",
      "To: 347,283\n",
      "Correcting Point: 141,323\n",
      "To: 136,323\n",
      "Correcting Point: 523,263\n",
      "To: 523,256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  33%|███▎      | 57/175 [00:22<00:41,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 550,284\n",
      "To: 550,285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  33%|███▎      | 58/175 [00:22<00:40,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 537,281\n",
      "To: 540,290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  34%|███▎      | 59/175 [00:23<00:40,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 534,281\n",
      "To: 537,290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  34%|███▍      | 60/175 [00:23<00:39,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 531,276\n",
      "To: 536,286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  35%|███▍      | 61/175 [00:23<00:40,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 530,275\n",
      "To: 537,283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  35%|███▌      | 62/175 [00:24<00:40,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 524,279\n",
      "To: 527,287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  36%|███▌      | 63/175 [00:24<00:39,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 522,280\n",
      "To: 525,287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  37%|███▋      | 64/175 [00:24<00:38,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 217,444\n",
      "To: 216,445\n",
      "Correcting Point: 525,281\n",
      "To: 528,286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  37%|███▋      | 65/175 [00:25<00:38,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 520,282\n",
      "To: 521,286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  38%|███▊      | 66/175 [00:25<00:38,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 298,283\n",
      "To: 298,282\n",
      "Correcting Point: 516,283\n",
      "To: 519,285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  42%|████▏     | 74/175 [00:28<00:34,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 295,406\n",
      "To: 295,410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  43%|████▎     | 75/175 [00:28<00:34,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 291,403\n",
      "To: 286,403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  43%|████▎     | 76/175 [00:28<00:34,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 287,403\n",
      "To: 285,403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  45%|████▍     | 78/175 [00:29<00:34,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 357,333\n",
      "To: 356,333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  45%|████▌     | 79/175 [00:30<00:33,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 280,410\n",
      "To: 276,414\n",
      "Correcting Point: 165,305\n",
      "To: 157,306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  46%|████▌     | 80/175 [00:30<00:33,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 280,401\n",
      "To: 277,403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  46%|████▋     | 81/175 [00:30<00:32,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 287,395\n",
      "To: 278,386\n",
      "Correcting Point: 361,328\n",
      "To: 361,332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  47%|████▋     | 82/175 [00:31<00:32,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 468,351\n",
      "To: 468,357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  47%|████▋     | 83/175 [00:31<00:32,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 498,331\n",
      "To: 499,331\n",
      "Correcting Point: 423,367\n",
      "To: 425,370\n",
      "Correcting Point: 304,307\n",
      "To: 305,307\n",
      "Correcting Point: 255,250\n",
      "To: 251,246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  48%|████▊     | 84/175 [00:31<00:31,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 220,315\n",
      "To: 221,315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  49%|████▊     | 85/175 [00:32<00:30,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 351,401\n",
      "To: 350,386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  49%|████▉     | 86/175 [00:32<00:30,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 356,398\n",
      "To: 356,386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  50%|████▉     | 87/175 [00:32<00:30,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 352,401\n",
      "To: 352,390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  50%|█████     | 88/175 [00:33<00:30,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 351,401\n",
      "To: 351,392\n",
      "Correcting Point: 357,244\n",
      "To: 358,240\n",
      "Correcting Point: 212,314\n",
      "To: 214,314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  52%|█████▏    | 91/175 [00:34<00:28,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 236,307\n",
      "To: 240,309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  53%|█████▎    | 92/175 [00:34<00:28,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 351,416\n",
      "To: 351,414\n",
      "Correcting Point: 217,307\n",
      "To: 207,306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  53%|█████▎    | 93/175 [00:34<00:28,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 350,416\n",
      "To: 350,415\n",
      "Correcting Point: 222,304\n",
      "To: 208,300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  54%|█████▎    | 94/175 [00:35<00:27,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 342,408\n",
      "To: 342,407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  54%|█████▍    | 95/175 [00:35<00:27,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 212,294\n",
      "To: 210,292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  55%|█████▍    | 96/175 [00:35<00:27,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 235,299\n",
      "To: 244,304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  55%|█████▌    | 97/175 [00:36<00:26,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 229,298\n",
      "To: 216,295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  57%|█████▋    | 99/175 [00:36<00:26,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 418,405\n",
      "To: 418,403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  58%|█████▊    | 101/175 [00:37<00:25,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 219,307\n",
      "To: 215,307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  58%|█████▊    | 102/175 [00:37<00:25,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 219,302\n",
      "To: 216,302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  59%|█████▉    | 103/175 [00:38<00:25,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 221,288\n",
      "To: 221,287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  61%|██████    | 106/175 [00:39<00:24,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 226,293\n",
      "To: 219,293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  61%|██████    | 107/175 [00:39<00:23,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 222,289\n",
      "To: 219,289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  62%|██████▏   | 108/175 [00:40<00:23,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 192,341\n",
      "To: 191,341\n",
      "Correcting Point: 477,395\n",
      "To: 477,387\n",
      "Correcting Point: 233,298\n",
      "To: 248,310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  62%|██████▏   | 109/175 [00:40<00:23,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 475,396\n",
      "To: 478,393\n",
      "Correcting Point: 231,275\n",
      "To: 214,279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  63%|██████▎   | 110/175 [00:40<00:23,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 473,396\n",
      "To: 477,396\n",
      "Correcting Point: 222,295\n",
      "To: 215,295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  63%|██████▎   | 111/175 [00:41<00:22,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 211,298\n",
      "To: 214,296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  64%|██████▍   | 112/175 [00:41<00:22,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 213,290\n",
      "To: 218,290\n",
      "Correcting Point: 234,290\n",
      "To: 219,295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  65%|██████▍   | 113/175 [00:41<00:22,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 188,313\n",
      "To: 168,289\n",
      "Correcting Point: 168,361\n",
      "To: 158,355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  65%|██████▌   | 114/175 [00:42<00:21,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 173,299\n",
      "To: 167,293\n",
      "Correcting Point: 132,408\n",
      "To: 131,408\n",
      "Correcting Point: 458,318\n",
      "To: 459,318\n",
      "Correcting Point: 512,277\n",
      "To: 512,276\n",
      "Correcting Point: 451,247\n",
      "To: 451,248\n",
      "Correcting Point: 168,257\n",
      "To: 166,252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  66%|██████▌   | 115/175 [00:42<00:21,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 257,421\n",
      "To: 257,422\n",
      "Correcting Point: 181,315\n",
      "To: 168,307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  66%|██████▋   | 116/175 [00:42<00:21,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 273,313\n",
      "To: 268,312\n",
      "Correcting Point: 242,383\n",
      "To: 235,386\n",
      "Correcting Point: 218,240\n",
      "To: 215,234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  67%|██████▋   | 117/175 [00:43<00:20,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 355,249\n",
      "To: 356,249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  67%|██████▋   | 118/175 [00:43<00:20,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 305,350\n",
      "To: 309,350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  68%|██████▊   | 119/175 [00:44<00:19,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 323,345\n",
      "To: 322,345\n",
      "Correcting Point: 400,190\n",
      "To: 409,192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  69%|██████▊   | 120/175 [00:44<00:19,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 302,298\n",
      "To: 300,296\n",
      "Correcting Point: 114,475\n",
      "To: 114,477\n",
      "Correcting Point: 236,295\n",
      "To: 236,292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  69%|██████▉   | 121/175 [00:44<00:19,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 93,397\n",
      "To: 91,397\n",
      "Correcting Point: 323,305\n",
      "To: 323,293\n",
      "Correcting Point: 239,295\n",
      "To: 237,293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  70%|██████▉   | 122/175 [00:45<00:18,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 311,309\n",
      "To: 310,295\n",
      "Correcting Point: 249,291\n",
      "To: 248,288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  70%|███████   | 123/175 [00:45<00:18,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 307,310\n",
      "To: 300,298\n",
      "Correcting Point: 247,290\n",
      "To: 247,288\n",
      "Correcting Point: 451,231\n",
      "To: 448,232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  71%|███████   | 124/175 [00:45<00:18,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 304,310\n",
      "To: 299,299\n",
      "Correcting Point: 247,291\n",
      "To: 249,292\n",
      "Correcting Point: 444,228\n",
      "To: 443,227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  71%|███████▏  | 125/175 [00:46<00:17,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 302,311\n",
      "To: 295,302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  72%|███████▏  | 126/175 [00:46<00:17,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 303,311\n",
      "To: 298,302\n",
      "Correcting Point: 249,291\n",
      "To: 249,289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  73%|███████▎  | 127/175 [00:46<00:16,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 311,309\n",
      "To: 307,299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  73%|███████▎  | 128/175 [00:47<00:16,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 312,310\n",
      "To: 309,300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  74%|███████▎  | 129/175 [00:47<00:15,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 309,312\n",
      "To: 305,305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  74%|███████▍  | 130/175 [00:47<00:15,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 314,312\n",
      "To: 313,306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  75%|███████▍  | 131/175 [00:48<00:15,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 314,312\n",
      "To: 314,307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  75%|███████▌  | 132/175 [00:48<00:14,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 310,316\n",
      "To: 310,315\n",
      "Correcting Point: 266,295\n",
      "To: 265,295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  76%|███████▌  | 133/175 [00:48<00:14,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 309,320\n",
      "To: 309,318\n",
      "Correcting Point: 266,297\n",
      "To: 266,296\n",
      "Correcting Point: 488,258\n",
      "To: 487,258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  82%|████████▏ | 143/175 [00:52<00:11,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 272,369\n",
      "To: 275,371\n",
      "Correcting Point: 628,167\n",
      "To: 630,165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  82%|████████▏ | 144/175 [00:52<00:10,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 270,370\n",
      "To: 268,373\n",
      "Correcting Point: 628,167\n",
      "To: 633,167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  83%|████████▎ | 145/175 [00:53<00:10,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 431,397\n",
      "To: 429,395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  83%|████████▎ | 146/175 [00:53<00:10,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 424,405\n",
      "To: 420,399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  84%|████████▍ | 147/175 [00:53<00:09,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 418,411\n",
      "To: 414,402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  85%|████████▍ | 148/175 [00:54<00:09,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 415,422\n",
      "To: 403,411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  85%|████████▌ | 149/175 [00:54<00:09,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 414,424\n",
      "To: 428,423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  86%|████████▌ | 150/175 [00:54<00:08,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 418,419\n",
      "To: 428,423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  86%|████████▋ | 151/175 [00:55<00:08,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 419,418\n",
      "To: 428,420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  87%|████████▋ | 152/175 [00:55<00:08,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 420,416\n",
      "To: 428,415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  87%|████████▋ | 153/175 [00:55<00:07,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 421,413\n",
      "To: 429,414\n",
      "Correcting Point: 505,284\n",
      "To: 505,286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  88%|████████▊ | 154/175 [00:56<00:07,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 422,414\n",
      "To: 428,416\n",
      "Correcting Point: 507,288\n",
      "To: 507,289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  89%|████████▊ | 155/175 [00:56<00:06,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 472,348\n",
      "To: 472,354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  89%|████████▉ | 156/175 [00:56<00:06,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 486,346\n",
      "To: 486,349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  90%|████████▉ | 157/175 [00:57<00:06,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 438,401\n",
      "To: 434,394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  90%|█████████ | 158/175 [00:57<00:05,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 414,414\n",
      "To: 408,407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  91%|█████████ | 159/175 [00:57<00:05,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 480,348\n",
      "To: 485,350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  91%|█████████▏| 160/175 [00:58<00:05,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 410,418\n",
      "To: 403,411\n",
      "Correcting Point: 488,384\n",
      "To: 480,383\n",
      "Correcting Point: 524,279\n",
      "To: 527,281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  92%|█████████▏| 161/175 [00:58<00:04,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 409,419\n",
      "To: 402,412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  93%|█████████▎| 162/175 [00:59<00:04,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 408,419\n",
      "To: 402,412\n",
      "Correcting Point: 527,276\n",
      "To: 531,279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  93%|█████████▎| 163/175 [00:59<00:04,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 408,418\n",
      "To: 402,412\n",
      "Correcting Point: 531,278\n",
      "To: 531,279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  94%|█████████▎| 164/175 [00:59<00:03,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 408,418\n",
      "To: 402,412\n",
      "Correcting Point: 532,277\n",
      "To: 533,278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  94%|█████████▍| 165/175 [01:00<00:03,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 422,412\n",
      "To: 418,402\n",
      "Correcting Point: 525,280\n",
      "To: 527,282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  95%|█████████▍| 166/175 [01:00<00:03,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 423,410\n",
      "To: 418,402\n",
      "Correcting Point: 499,286\n",
      "To: 495,296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  95%|█████████▌| 167/175 [01:00<00:02,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 416,415\n",
      "To: 411,407\n",
      "Correcting Point: 519,279\n",
      "To: 525,284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  96%|█████████▌| 168/175 [01:01<00:02,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 419,414\n",
      "To: 415,404\n",
      "Correcting Point: 396,175\n",
      "To: 394,175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  97%|█████████▋| 169/175 [01:01<00:02,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 487,345\n",
      "To: 488,345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  98%|█████████▊| 172/175 [01:02<00:01,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 285,224\n",
      "To: 289,227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image:  99%|█████████▉| 173/175 [01:02<00:00,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting Point: 274,228\n",
      "To: 277,231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image: 100%|██████████| 175/175 [01:03<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating video\n"
     ]
    }
   ],
   "source": [
    "masks_refinement_bbox, logits_refinement_bbox, painted_images_refinement_bbox = run_model_on_ovis_set(name = '_mask_pointsPN_lizard',model = modelBothNeg, path_set = ovis_images,videos = vidTrain[37:38],annotations = annTrain,compute_metrics = False, save_masks=False, compute_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_refinement_bbox, logits_refinement_bbox, painted_images_refinement_bbox = run_model_on_ovis_set(name = '_both_TA_lizard',model = modelBoth, path_set = ovis_images,videos = vidTrain[37:38],annotations = annTrain,compute_metrics = False, save_masks=False, compute_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, logits, painted_images = run_model_on_ovis_set(name = '_no_ref_lizards',model = model, path_set = ovis_images,videos = vidTrain[37:38],annotations = annTrain,compute_metrics = False, save_masks=False, compute_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_refinement_point, logits_refinement_point, painted_images_refinement_point = run_model_on_ovis_set(name = '_point_ref_lizard',model = modelSamPoint, path_set = ovis_images,videos = vidTrain[37:38],annotations = annTrain,compute_metrics = False, save_masks=False, compute_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_images(image1, image2, image3):\n",
    "    fig = plt.figure(figsize=(30, 15))\n",
    "\n",
    "    # Create a grid with two subplots\n",
    "    grid = plt.GridSpec(1, 3)\n",
    "\n",
    "    # Display the first image in the left subplot\n",
    "    ax1 = plt.subplot(grid[0])\n",
    "    ax1.imshow(image1)\n",
    "    ax1.set_title('Image')\n",
    "\n",
    "    # Display the first image in the left subplot\n",
    "    ax1 = plt.subplot(grid[1])\n",
    "    ax1.imshow(image2)\n",
    "    ax1.set_title('No Ref')\n",
    "\n",
    "    # Display the second image in the right subplot\n",
    "    ax2 = plt.subplot(grid[2])\n",
    "    ax2.imshow(image3)\n",
    "    ax2.set_title('Ref')\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.subplots_adjust(wspace=0.1)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images_from_folder(ovis_images,vidTrain[37]['file_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(153,157):\n",
    "    plt.imshow(images[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in range(0,10):\n",
    " print_images(images[id],painted_images[id],painted_images_refinement_bbox[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in painted_images_refinement_point: \n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks_refinement_point[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in painted_images: \n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUNK TESTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images_from_folder(ovis_images,vidTrain[0]['file_names'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_point_of_interest(segmentation_mask):\n",
    "    # Find contours in the segmentation mask\n",
    "    points = []\n",
    "    contours, _ = cv2.findContours(segmentation_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for contour in contours:\n",
    "        # Extract the bounding box coordinates of the contour\n",
    "        M = cv2.moments(contour)\n",
    "        points.append([int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"])])\n",
    "\n",
    "    return np.array(points).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = [a for a in annTrain if a['video_id'] == vidTrain[0]['id']]\n",
    "masks = [(annToMask(a, 0) * (i + 1)) for i, a in enumerate(ann)]\n",
    "initial_mask = unifyMasks(masks, vidTrain[0]['width'], vidTrain[0]['height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_point_of_interest(masks[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = []\n",
    "for mask in masks:\n",
    "    \n",
    "    points = get_best_point_of_interest(mask)\n",
    "    all_points.append(points)\n",
    "    plt.imshow(mask)\n",
    "    print(points[0][0])\n",
    "    print(all_points)\n",
    "    plt.scatter(points[0][0], points[0][1], c='red', marker='o')\n",
    "\n",
    "    # Set the axis limits\n",
    "    plt.xlim(0, mask.shape[1])\n",
    "    plt.ylim(mask.shape[0], 0)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = np.array(all_points).astype('uint8')\n",
    "all_labels = np.ones((all_points.shape[0],1)).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.reset_image()\n",
    "modelSam.xmem.sam_model.sam_controler.set_image(images[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.predictor.predict(point_coords=all_points[0], \n",
    "                                point_labels=all_labels[0], \n",
    "                                multimask_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'point'\n",
    "prompts = {\n",
    "    'point_coords': all_points[0],\n",
    "    'point_labels': all_labels[0], \n",
    "}\n",
    "modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.predictor.predict(point_coords=all_points, \n",
    "                                point_labels=all_labels, \n",
    "                                multimask_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM_checkpoint = \"./checkpoints/sam_vit_h_4b8939.pth\"\n",
    "xmem_checkpoint = \"./checkpoints/XMem-s012.pth\"\n",
    "e2fgvi_checkpoint = \"./checkpoints/E2FGVI-HQ-CVPR22.pth\"\n",
    "args = {'use_refinement':False}\n",
    "model = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)\n",
    "args = {'use_refinement':True}\n",
    "modelSam = TrackingAnything(SAM_checkpoint, xmem_checkpoint, e2fgvi_checkpoint,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images_from_folder(ovis_images,vidTrain[0]['file_names'])\n",
    "#model.xmem.sam_refinement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = [a for a in annTrain if a['video_id'] == vidTrain[0]['id']]\n",
    "masks = [(annToMask(a, 0) * (i + 1)) for i, a in enumerate(ann)]\n",
    "initial_mask = unifyMasks(masks, vidTrain[0]['width'], vidTrain[0]['height'])\n",
    "masks = [(annToMask(a, 0)) for i, a in enumerate(ann)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resized_mask = cv2.resize(masks[3], (256, 256), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(masks[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bounding_box(segmentation_mask):\n",
    "    # Get the indices where the segmentation mask is non-zero\n",
    "    nonzero_indices = np.nonzero(segmentation_mask)\n",
    "    \n",
    "    # Calculate the bounding box coordinates\n",
    "    min_row = np.min(nonzero_indices[0])\n",
    "    max_row = np.max(nonzero_indices[0])\n",
    "    min_col = np.min(nonzero_indices[1])\n",
    "    max_col = np.max(nonzero_indices[1])\n",
    "    \n",
    "    # Return the bounding box coordinates as a tuple\n",
    "    bounding_box = [min_col,min_row, max_col, max_row]\n",
    "    return bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = [compute_bounding_box(mask) for mask in masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = torch.tensor(bboxes, device=modelSam.xmem.sam_model.sam_controler.predictor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_boxes = modelSam.xmem.sam_model.sam_controler.predictor.transform.apply_boxes_torch(input_boxes, images[0].shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.reset_image()\n",
    "modelSam.xmem.sam_model.sam_controler.set_image(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0])\n",
    "plt.show()\n",
    "# mask only ------------------------\n",
    "mode = 'bounding_boxes'\n",
    "prompts = {'bounding_boxes': transformed_boxes}\n",
    "\n",
    "masksout, scores, logits = modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=False)  # masks (n, h, w), scores (n,), logits (n, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masksout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_mask = np.zeros_like(masksout[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(pp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masksout[0] * 1 + masksout[1] * 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_mask[masksout[1]] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masksout[0] == pp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(scores)):\n",
    "    if i == np.argmax(scores): print('Selected')\n",
    "    painted_image = mask_painter(images[0], masksout[i][0].numpy().astype('uint8'))\n",
    "    show_box(input_boxes[i], plt.gca())\n",
    "    plt.imshow(painted_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(images[0])\n",
    "plt.show()\n",
    "# mask only ------------------------\n",
    "mode = 'bbox'\n",
    "prompts = {'bounding_box': transformed_boxes}\n",
    "\n",
    "masksout, scores, logits = modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=False)  # masks (n, h, w), scores (n,), logits (n, 256, 256)\n",
    "for i in range(0,len(scores)):\n",
    "    if i == np.argmax(scores): print('Selected')\n",
    "    painted_image = mask_painter(images[1], masksout[i].astype('uint8'))\n",
    "    show_box(bb, plt.gca())\n",
    "    plt.imshow(painted_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.predictor.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelSam.xmem.sam_model.sam_controler.reset_image()\n",
    "#modelSam.xmem.sam_model.sam_controler.set_image(images[0])\n",
    "plt.imshow(images[0])\n",
    "\n",
    "# mask only ------------------------\n",
    "mode = 'mask'\n",
    "prompts = {'mask_input': masks[3][None,:,:]}\n",
    "\n",
    "masks, scores, logits = modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=True)  # masks (n, h, w), scores (n,), logits (n, 256, 256)\n",
    "for i in range(0,len(scores)):\n",
    "    if i == np.argmax(scores): print('Selected')\n",
    "    painted_image = mask_painter(images[1], masks[i].astype('uint8'))\n",
    "    plt.imshow(painted_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.xmem.clear_memory()\n",
    "masksout, logitsout, painted_imagesout = model.generator(images=images[0:2], template_mask=initial_mask)\n",
    "model.xmem.clear_memory() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "resizer = Resize([256, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triallogits = logitsout[1][0].unsqueeze(0)\n",
    "ind_logits = resizer(triallogits).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painted_imagesout[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.reset_image()\n",
    "modelSam.xmem.sam_model.sam_controler.set_image(images[0])\n",
    "plt.imshow(images[0])\n",
    "\n",
    "# mask only ------------------------\n",
    "mode = 'mask'\n",
    "prompts = {'mask_input': ind_logits}\n",
    "\n",
    "masks, scores, logits = modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=True)  # masks (n, h, w), scores (n,), logits (n, 256, 256)\n",
    "for i in range(0,len(scores)):\n",
    "    if i == np.argmax(scores): print('Selected')\n",
    "    painted_image = mask_painter(images[1], masks[i].astype('uint8'))\n",
    "    plt.imshow(painted_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSam.xmem.sam_model.sam_controler.reset_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelSam.xmem.sam_model.sam_controler.set_image(images[0])\n",
    "#model.samcontroler.sam_controler.set_image(images[0])\n",
    "mode = 'point'\n",
    "prompts = {\n",
    "    'point_coords': np.array([[500, 650]]),\n",
    "    'point_labels': np.array([1]), \n",
    "}\n",
    "masks, scores, logits = modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=False)  # masks (n, h, w), scores (n,), logits (n, 256, 256)\n",
    "painted_image = mask_painter(images[0], masks[np.argmax(scores)].astype('uint8'))\n",
    "#cv2.imwrite('/hhd3/gaoshang/truck_point.jpg', painted_image)\n",
    "\n",
    "\n",
    "plt.imshow(painted_image)\n",
    "plt.show()\n",
    "\n",
    "# mask only ------------------------\n",
    "mode = 'mask'\n",
    "mask_input  = logits[np.argmax(scores), :, :]\n",
    "\n",
    "prompts = {'mask_input': mask_input[None, :, :]}\n",
    "print(prompts['mask_input'].shape)\n",
    "\n",
    "masks, scores, logits = modelSam.xmem.sam_model.sam_controler.predict(prompts, mode, multimask=True)  # masks (n, h, w), scores (n,), logits (n, 256, 256)\n",
    "for i in range(0,len(scores)):\n",
    "    if i == np.argmax(scores): print('Selected')\n",
    "    painted_image = mask_painter(images[0], masks[i].astype('uint8'))\n",
    "    plt.imshow(painted_image)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee una imagen y la muestra en una ventanaovis_images + first_video_folder +'/img_0000001.jpg'\n",
    "img = cv2.imread(os.path.join(ovis_images + first_video_folder +'/img_0000001.jpg'))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    (255, 0, 0),    # Rojo\n",
    "    (0, 255, 0),    # Verde\n",
    "    (0, 0, 255),    # Azul\n",
    "    (255, 255, 0),  # Amarillo\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (0, 255, 255),  # Cian\n",
    "    (128, 0, 0),    # Marrón oscuro\n",
    "    (0, 128, 0),    # Verde oscuro\n",
    "    (0, 0, 128),    # Azul oscuro\n",
    "    (128, 128, 0),  # Amarillo oscuro\n",
    "    (128, 0, 128),  # Magenta oscuro\n",
    "    (0, 128, 128),  # Cian oscuro\n",
    "    (255, 128, 0),  # Naranja\n",
    "    (128, 255, 0),  # Lima\n",
    "    (255, 0, 128),  # Rosa\n",
    "    (128, 0, 255),  # Violeta\n",
    "    (0, 255, 128),  # Turquesa\n",
    "    (0, 128, 255),  # Azul claro\n",
    "    (255, 128, 128), # Rosa claro\n",
    "    (128, 255, 128)  # Verde claro\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = []\n",
    "video = vidTrain[0]\n",
    "for a in annTrain:\n",
    "    if a['video_id'] == video['id']:\n",
    "        ann.append(a)\n",
    "        break\n",
    "    else: continue\n",
    "\n",
    "mask = annToMask(ann[0], 0)\n",
    "colored_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "colored_mask[:, :, 0] = mask * colors[0][0]\n",
    "colored_mask[:, :, 1] = mask * colors[0][1]\n",
    "colored_mask[:, :, 2] = mask * colors[0][2]\n",
    "plt.imshow(colored_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = []\n",
    "for a in annTrain:\n",
    "    if a['video_id'] == video['id']:\n",
    "        ann.append(a)\n",
    "    else: continue\n",
    "\n",
    "masks = []\n",
    "for i,a in enumerate(ann):\n",
    "    m = annToMask(a, 0)\n",
    "    m = m * (i + 1)\n",
    "    masks.append(m)\n",
    "\n",
    "w, h = video['width'], video['height']\n",
    "unified = unifyMasks(masks, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.xmem.clear_memory()\n",
    "masks, logits, painted_images = model.generator(images=images[0:5], template_mask=unified)\n",
    "model.xmem.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(painted_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rgb_mask(mask):\n",
    "    colored_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    colored_mask[:, :, 0] = mask * colors[0][0]\n",
    "    colored_mask[:, :, 1] = mask * colors[0][1]\n",
    "    colored_mask[:, :, 2] = mask * colors[0][2]\n",
    "    plt.imshow(colored_mask)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rgb_mask(logits[29])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
