{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gdown\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(sys.path[0]+\"/tracker\")\n",
    "sys.path.append(sys.path[0]+\"/tracker/model\")\n",
    "from track_anything import TrackingAnything\n",
    "from track_anything import parse_augment\n",
    "import requests\n",
    "import json\n",
    "import torchvision\n",
    "import torch \n",
    "from tools.painter import mask_painter\n",
    "import psutil\n",
    "import time\n",
    "try: \n",
    "    from mmcv.cnn import ConvModule\n",
    "except:\n",
    "    os.system(\"mim install mmcv\")\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools import mask as maskUtils\n",
    "from PIL import Image\n",
    "from dataset.dataset import DAVIS_MO_Test\n",
    "from dataset.longdataset import LongVideoDataset\n",
    "from dataset.errorfunctions import db_eval_boundary,db_eval_iou,seg2bmap\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import random\n",
    "import string\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'posix':\n",
    "    ovis_anotations = '../data.nosync/OVIS/annotations/'\n",
    "    ovis_images = '../data.nosync/OVIS/train_images/'\n",
    "    davis_root = '../data.nosync/DAVIS2017'\n",
    "    longdataset_root = '../data.nosync/LongDataset/'\n",
    "    longvos_root = '../data.nosync/LongVOS/test'\n",
    "else:\n",
    "    ovis_anotations = 'D:/HADA/data/OVIS/annotations/'\n",
    "    ovis_images = 'D:/HADA/data/OVIS/train_images/'\n",
    "    davis_root = 'D:\\HADA\\data\\DAVIS'\n",
    "    longdataset_root = 'D:/HADA/data/LongDataset/'\n",
    "    longvos_root = 'D:\\HADA\\data/LongVOS/test'\n",
    "\n",
    "\n",
    "all_tests_csv = './result/all_tests.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de datos. (Anotaciones, mascaras e imagenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargarDatos(ruta_ann):\n",
    "    with open(ruta_ann + 'annotations_train.json') as f:\n",
    "        annotationsTrain = json.load(f)\n",
    "\n",
    "    with open(ruta_ann + 'annotations_valid.json') as f:\n",
    "        annotationsValid = json.load(f)\n",
    "\n",
    "    with open(ruta_ann + 'annotations_test.json') as f:\n",
    "        annotationsTest = json.load(f)\n",
    "\n",
    "    clases = annotationsTrain['categories']\n",
    "    vidTrain = annotationsTrain['videos']\n",
    "    annTrain = annotationsTrain['annotations']\n",
    "    vidValid = annotationsValid['videos']\n",
    "    annValid = annotationsValid['annotations']\n",
    "    vidTest = annotationsTest['videos']\n",
    "    annTest = annotationsTest['annotations']\n",
    "\n",
    "    return clases, vidTrain, annTrain, vidValid, annValid, vidTest, annTest\n",
    "\n",
    "clases, vidTrain, annTrain, vidValid, annValid, vidTest, annTest = cargarDatos(ovis_anotations)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annToRLE(ann, frameId):\n",
    "    \"\"\"\n",
    "    Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "    :return: binary mask (numpy 2D array)\n",
    "    \"\"\"\n",
    "    h, w = ann['height'], ann['width']\n",
    "    segm = ann['segmentations'][frameId]\n",
    "    if segm is None:\n",
    "        return None\n",
    "    if type(segm) == \"list\":\n",
    "        # polygon -- a single object might consist of multiple parts\n",
    "        # we merge all parts into one mask rle code\n",
    "        rles = maskUtils.frPyObjects(segm, h, w)\n",
    "        rle = maskUtils.merge(rles)\n",
    "    elif type(segm['counts']) == \"list\":\n",
    "        # uncompressed RLE\n",
    "        rle = maskUtils.frPyObjects(segm, h, w)\n",
    "    else:\n",
    "        # rle\n",
    "        rle = segm\n",
    "    return rle\n",
    "\n",
    "\n",
    "def annToMask(ann, frameId):\n",
    "    \"\"\"\n",
    "    Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "    :return: binary mask (numpy 2D array)\n",
    "    \"\"\"\n",
    "    rle = annToRLE(ann, frameId)\n",
    "    if rle is not None:\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m\n",
    "\n",
    "\n",
    "\n",
    "def combineMasks(masks, width, height):\n",
    "    # Crear una matriz vacía para la máscara combinada\n",
    "    combined = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Combinar las máscaras en la matriz vacía\n",
    "    for mask in masks:\n",
    "        combined += mask  # Sumar la máscara a la máscara combinada\n",
    "\n",
    "    # Aplicar umbral para obtener una única máscara binaria\n",
    "    combined = np.where(combined > 0, 1, 0)\n",
    "    return combined\n",
    "\n",
    "def unifyMasks(masks, width, height):\n",
    "    # Crear una matriz vacía para la máscara combinada\n",
    "    unified = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Combinar las máscaras en la matriz vacía\n",
    "    for mask in masks:\n",
    "        unified += mask  # Sumar la máscara a la máscara combinada\n",
    "\n",
    "    \n",
    "    return unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(path,image_files):\n",
    "    images = []\n",
    "    for file in image_files:\n",
    "        img = cv2.imread(os.path.join(path,file))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "def load_all_images_davis(loader,video_info):\n",
    "    name, frames, objects = video_info\n",
    "    all_images, all_frames = [],[]\n",
    "    for i in range(0,frames):\n",
    "        F_last,M_last = loader.load_single_image(name,i)\n",
    "        all_images.append((np.array(F_last[:,0]).transpose(1, 2, 0)* 255.).astype(np.uint8))\n",
    "        all_frames.append(np.array(M_last[1:objects+1,0]).astype(np.uint8))\n",
    "    return all_images,all_frames\n",
    "\n",
    "def load_all_initial_masks_from_dataset():\n",
    "    all_masks = []\n",
    "    for video in vidTrain:\n",
    "        ann = [a for a in annTrain if a['video_id'] == video['id']]\n",
    "        masks = [annToMask(a, 0) * (i + 1) for i, a in enumerate(ann) if annToMask(a, 0) is not None]\n",
    "        all_masks.append(unifyMasks(masks, video['width'], video['height']))\n",
    "    return all_masks\n",
    "\n",
    "def load_all_masks_for_video(video,num_masks):\n",
    "    ann = [a for a in annTrain if a['video_id'] == video['id']]\n",
    "    all_masks  = []\n",
    "    for image_num in range(0,video['length']):\n",
    "        masks = []\n",
    "        for i in range(0, num_masks):\n",
    "            annot = annToMask(ann[i], image_num)\n",
    "            if annot is not None: masks.append(annot * (i + 1))\n",
    "        single_mask = unifyMasks(masks, video['width'], video['height'])\n",
    "        all_masks.append(single_mask)\n",
    "    return all_masks\n",
    "\n",
    "def pad_to_divisible_by_two(frames):\n",
    "    max_height = max(frame.shape[0] for frame in frames)\n",
    "    max_width = max(frame.shape[1] for frame in frames)\n",
    "    new_height = max_height + 1 if max_height % 2 != 0 else max_height\n",
    "    new_width = max_width + 1 if max_width % 2 != 0 else max_width\n",
    "\n",
    "    padded_frames = []\n",
    "    for frame in frames:\n",
    "        height_pad = new_height - frame.shape[0]\n",
    "        width_pad = new_width - frame.shape[1]\n",
    "        padded_frame = np.pad(frame, ((0, height_pad), (0, width_pad), (0, 0)), mode='constant')\n",
    "        padded_frames.append(padded_frame)\n",
    "\n",
    "    return padded_frames\n",
    "\n",
    "def generate_video_from_frames(frames, output_path, fps=30):\n",
    "    frames = torch.from_numpy(np.asarray(frames))\n",
    "    if not os.path.exists(os.path.dirname(output_path)):\n",
    "        os.makedirs(os.path.dirname(output_path))\n",
    "    torchvision.io.write_video(output_path, frames, fps=fps, video_codec=\"libx264\")\n",
    "    return output_path\n",
    "\n",
    "def select_smaller_set(videos, number_videos = 100):\n",
    "    size = [(video['id'],video['height']*video['height']*len(video['file_names'])) for video in videos]\n",
    "    #size = [(video['id'],video['height']*video['height']) for video in videos]\n",
    "    sorted_list = sorted(size, key=lambda x: x[1])\n",
    "    listed = [tup[0] for tup in sorted_list[:number_videos]]\n",
    "    filtered_list = [d for id_ in listed for d in vidTrain if d.get('id') == id_]\n",
    "    return filtered_list\n",
    "\n",
    "def select_bigger_set(videos, size = 100):\n",
    "    size = [(video['id'],video['height']*video['height']*len(video['file_names'])) for video in videos]\n",
    "    sorted_list = sorted(size, key=lambda x: x[1])\n",
    "    listed = [tup[0] for tup in sorted_list[::-1][:100]]\n",
    "    filtered_list = [d for id_ in listed for d in vidTrain if d.get('id') == id_]\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MÉTRICAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_real_iou(mask1, mask2):\n",
    "    # Ensure both masks have the same shape\n",
    "    assert mask1.shape == mask2.shape, \"Mask shapes must be the same.\"\n",
    "\n",
    "    # Calculate intersection and union for each label\n",
    "    #labels = np.unique(np.concatenate((mask1, mask2)))[1:]\n",
    "    labels =np.unique(mask2)\n",
    "    \n",
    "    iou_per_label = {}\n",
    "    iou_values = []\n",
    "\n",
    "    for label in labels:\n",
    "        mask1_label = mask1 == label\n",
    "        mask2_label = mask2 == label\n",
    "        iou = db_eval_iou(mask1_label,mask2_label)\n",
    "        iou_per_label[label] = iou\n",
    "        iou_values.append(iou)\n",
    "\n",
    "    # Calculate IoU as total of the mask \n",
    "    #iou = np.sum(intersection) / np.sum(union)\n",
    "    iou = np.nanmean(iou_values)\n",
    "    \n",
    "    # Calculate IoU as mean of objects\n",
    "    iou_mean_object = sum(iou_per_label.values()) / len(iou_per_label)\n",
    "\n",
    "    return iou,iou_mean_object, iou_per_label\n",
    "\n",
    "def compute_f_score(true_positives, false_positives, false_negatives):\n",
    "    divider = (true_positives + false_positives)\n",
    "    precision = (true_positives / divider) if divider != 0 else 0\n",
    "\n",
    "    divider = (true_positives + false_negatives)\n",
    "    recall = (true_positives / divider) if divider != 0 else 0\n",
    "\n",
    "    divider = (precision + recall)\n",
    "    f_measure = (2 * (precision * recall) / divider) if divider != 0 else 0\n",
    "    return f_measure\n",
    "\n",
    "def compute_f_measure(mask1, mask2):\n",
    "    # Ensure both masks have the same shape\n",
    "    assert mask1.shape == mask2.shape, \"Mask shapes must be the same.\"\n",
    "\n",
    "    # Calculate F-measure for each label\n",
    "    #labels = np.unique(np.concatenate((mask1, mask2)))[1:]\n",
    "    labels =np.unique(mask2)\n",
    "    f_measure_per_label = {}\n",
    "    add_true_positives = 0\n",
    "    add_false_positives = 0\n",
    "    add_false_negatives = 0\n",
    "\n",
    "    for label in labels:\n",
    "        mask1_label = mask1 == label\n",
    "        mask2_label = mask2 == label\n",
    "\n",
    "        true_positives = np.logical_and(mask1_label, mask2_label).sum()\n",
    "        false_positives = np.logical_and(mask1_label, np.logical_not(mask2_label)).sum()\n",
    "        false_negatives = np.logical_and(np.logical_not(mask1_label), mask2_label).sum()\n",
    "\n",
    "        add_true_positives += true_positives\n",
    "        add_false_positives += false_positives\n",
    "        add_false_negatives += false_negatives\n",
    "\n",
    "        f_measure = compute_f_score(true_positives, false_positives, false_negatives)\n",
    "        f_measure_per_label[label] = f_measure\n",
    "\n",
    "    # Calculate F Measure as total of the mask \n",
    "    overall_f_measure = compute_f_score(add_true_positives, add_false_positives, add_false_negatives)\n",
    "\n",
    "    # Calculate IoU as mean of objects\n",
    "    f_mean_object = sum(f_measure_per_label.values()) / len(f_measure_per_label)\n",
    "\n",
    "    return overall_f_measure,f_mean_object,f_measure_per_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(mask1, mask2):\n",
    "    # Ensure both masks have the same shape\n",
    "    assert mask1.shape == mask2.shape, \"Mask shapes must be the same.\"\n",
    "\n",
    "    # Calculate intersection and union for each label\n",
    "    labels = np.unique(np.concatenate((mask1, mask2)))[1:]\n",
    "    #labels =np.intersect1d(np.unique(mask1), np.unique(mask2))[1:]\n",
    "\n",
    "    labels =np.unique(mask2)[1:]\n",
    "    intersection = np.zeros_like(mask1, dtype=np.float32)\n",
    "    union = np.zeros_like(mask1, dtype=np.float32)\n",
    "    iou_per_label = {}\n",
    "\n",
    "    for label in labels:\n",
    "        mask1_label = mask1 == label\n",
    "        mask2_label = mask2 == label\n",
    "        c_intersection = np.logical_and(mask1_label, mask2_label)\n",
    "        c_union = np.logical_or(mask1_label, mask2_label)\n",
    "        intersection += c_intersection\n",
    "        union += c_union\n",
    "        iou_per_label[label] = np.sum(c_intersection) / np.sum(c_union)\n",
    "    # Calculate IoU as total of the mask \n",
    " \n",
    "    iou = np.sum(intersection) / np.sum(union) if np.sum(union) != 0 else np.nan\n",
    "    #aaa iou = np.sum(intersection) / np.sum(union) \n",
    "    \n",
    "    \n",
    "    # Calculate IoU as mean of objects\n",
    "    if len(iou_per_label) > 0: \n",
    "        iou_mean_object = sum(iou_per_label.values()) / len(iou_per_label)\n",
    "    else: iou_mean_object  = 0\n",
    "\n",
    "    #print('J_Measure->OverAll, Object Mean, Per label',iou,iou_mean_object, iou_per_label)\n",
    "    return iou,iou_mean_object, iou_per_label\n",
    "\n",
    "\n",
    "def compute_real_f_measure(mask1, mask2): #(mask_infered,mask_gt)\n",
    "    # Ensure both masks have the same shape\n",
    "    assert mask1.shape == mask2.shape, \"Mask shapes must be the same.\"\n",
    "\n",
    "    # Calculate F-measure for each label\n",
    "    labels = np.unique(np.concatenate((mask1, mask2)))[1:]\n",
    "    labels =np.unique(mask2)[1:]\n",
    "    f_measure_per_label = {}\n",
    "    f_measures = []\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "\n",
    "    for label in labels:\n",
    "        mask1_label = mask1 == label\n",
    "        mask2_label = mask2 == label\n",
    "\n",
    "        f_measure,precision,recall = db_eval_boundary(mask1_label, mask2_label)\n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "        f_measure_per_label[label] = f_measure\n",
    "        f_measures.append(f_measure)\n",
    "\n",
    "    # Calculate F Measure as total of the mask \n",
    "\n",
    "    overall_f_measure = np.nanmean(f_measures) if len(f_measures) != 0 else np.nan  \n",
    "    #aaa overall_f_measure = np.nanmean(f_measures)    \n",
    "\n",
    "   \n",
    "    # Calculate IoU as mean of objects\n",
    "    f_mean_object = sum(f_measure_per_label.values()) / len(f_measure_per_label) if len(f_measure_per_label) != 0 else 0.0\n",
    "    #aaaf_mean_object = sum(f_measure_per_label.values()) / len(f_measure_per_label)\n",
    "\n",
    "    return overall_f_measure,f_mean_object,f_measure_per_label\n",
    "\n",
    "def split_dict_list_to_lists(dict_list):\n",
    "\n",
    "    key_lists = {}\n",
    "    for dictionary in dict_list:\n",
    "        for key, value in dictionary.items():\n",
    "            if key in key_lists:\n",
    "                key_lists[key].append(value)\n",
    "            else:\n",
    "                key_lists[key] = [value]\n",
    "    result = [values_list for _, values_list in key_lists.items()]\n",
    "    keys_in_order = list(key_lists.keys())\n",
    "    return result, keys_in_order\n",
    "\n",
    "def add_dict(list_of_dicts):\n",
    "    mean_dict = {}\n",
    "    key_counts = {}\n",
    "\n",
    "    for d in list_of_dicts:\n",
    "        for key, value in d.items():\n",
    "            mean_dict[key] = mean_dict.get(key, 0) + value\n",
    "            key_counts[key] = key_counts.get(key, 0) + 1\n",
    "\n",
    "    for key in mean_dict:\n",
    "        mean_dict[key] /= key_counts[key]\n",
    "    return mean_dict\n",
    "\n",
    "def db_statistics(per_frame_values):\n",
    "    \"\"\" Compute mean,recall and decay from per-frame evaluation.\n",
    "    Arguments:\n",
    "        per_frame_values (ndarray): per-frame evaluation\n",
    "\n",
    "    Returns:\n",
    "        M,O,D (float,float,float):\n",
    "            return evaluation statistics: mean,recall,decay.\n",
    "    \"\"\"\n",
    "\n",
    "    # strip off nan values\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        M = np.nanmean(per_frame_values)\n",
    "        O = np.nanmean(per_frame_values > 0.5)\n",
    "\n",
    "    N_bins = 4\n",
    "    ids = np.round(np.linspace(1, len(per_frame_values), N_bins + 1) + 1e-10) - 1\n",
    "    ids = ids.astype(np.uint8)\n",
    "\n",
    "    D_bins = [per_frame_values[ids[i]:ids[i + 1] + 1] for i in range(0, 4)]\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        D = np.nanmean(D_bins[0]) - np.nanmean(D_bins[3])\n",
    "\n",
    "    return M, O, D\n",
    "def compute_statistics_per_label(args): \n",
    "    f_per_objectframe_list,list_of_keys = args\n",
    "    metrics_dict = {}\n",
    "    for metrics,key in zip (f_per_objectframe_list,list_of_keys):\n",
    "        metrics_dict[f'{key}'] = db_statistics(np.array(metrics))\n",
    "    return metrics_dict\n",
    "\n",
    "def compute_all_video_metrics(name,masks,ground_truth_masks,df_per_frame_metrics):\n",
    "    f_measure_lst,f_measure_object_lst, f_measure_per_label_lst, iou_lst, iou_object_lst, iou_per_label_lst  =  [], [], [], [], [], []\n",
    "    for i,(mask_infered, mask_gt) in enumerate(zip(masks,ground_truth_masks)):\n",
    "            #print(f'Frame {i+1}: Real Values {len(np.unique(mask_gt)) - 1}, Values Infered {len(np.unique(mask_infered)) - 1}  ')\n",
    "            f_measure,f_measure_object, f_measure_per_label = compute_real_f_measure(mask_infered,mask_gt)\n",
    "            #f_real_measure,f_real_measure_object, f_real_measure_per_label = compute_real_f_measure(mask_infered, mask_gt)\n",
    "            #print(f'F measure: {f_measure}, F measure per object {f_measure_object}, F measure {f_measure_per_label}')\n",
    "            #print(f'REAL: F measure: {f_real_measure}, F measure per object {f_real_measure_object}, F measure {f_real_measure_per_label}')\n",
    "            iou,iou_object, iou_per_label = calculate_iou(mask_infered,mask_gt)\n",
    "            #iou_real,iou_object_real, iou_per_label_real = calculate_real_iou(mask_infered,mask_gt)\n",
    "            #print(f'Iou measure: {iou}, Iou measure per object {iou_object}, Iou measure {iou_per_label}')\n",
    "            #print(f'REAL Iou measure: {iou_real}, Iou measure per object {iou_object_real}, Iou measure {iou_per_label_real}')\n",
    "            df_per_frame_metrics.loc[len(df_per_frame_metrics)] = np.array([name,i + 1,f_measure,iou,f_measure_object,iou_object,f_measure_per_label,iou_per_label])\n",
    "            #print(f'Mask {i + 1}: f_mesure {f_measure}, per label {f_measure_per_label}, iou {iou}, per label {iou_per_label}')\n",
    "\n",
    "            f_measure_lst.append(f_measure)\n",
    "            f_measure_object_lst.append(f_measure_object)\n",
    "            f_measure_per_label_lst.append(f_measure_per_label)\n",
    "            \n",
    "            iou_lst.append(iou)\n",
    "            iou_object_lst.append(iou_object)\n",
    "            iou_per_label_lst.append(iou_per_label)\n",
    "    \n",
    "    f_statistics = db_statistics(np.array(f_measure_lst))\n",
    "    j_statistics = db_statistics(np.array(iou_lst))\n",
    "\n",
    "    f_statistics_object = db_statistics(np.array(f_measure_object_lst))\n",
    "    j_statistics_object = db_statistics(np.array(iou_object_lst))\n",
    "\n",
    "    f_statistics_per_label = compute_statistics_per_label(split_dict_list_to_lists(f_measure_per_label_lst))\n",
    "    j_statistics_per_label = compute_statistics_per_label(split_dict_list_to_lists(iou_per_label_lst))\n",
    "    return f_statistics,j_statistics,f_statistics_object,j_statistics_object, f_statistics_per_label, j_statistics_per_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(mask1, mask2):\n",
    "    \"\"\"Compute Intersection over Union (IoU) for two binary masks.\"\"\"\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    return intersection / union\n",
    "\n",
    "def compute_auc_interpolated(precisions, recalls):\n",
    "    sorted_indices = np.argsort(recalls)\n",
    "    sorted_recalls = np.array(recalls)[sorted_indices]\n",
    "    sorted_precisions = np.array(precisions)[sorted_indices]\n",
    "    \n",
    "    interpolated_precisions = np.maximum.accumulate(sorted_precisions[::-1])[::-1]\n",
    "    area = np.trapz(interpolated_precisions, sorted_recalls)\n",
    "    \n",
    "    return area\n",
    "\n",
    "\n",
    "\n",
    "def true_positives(gt_masks, pred_masks, scores):\n",
    "\n",
    "    iou_thresholds = [round(v,2) for v in np.arange(0.5, 1.0, 0.05)]\n",
    "    columns = ['Frame','Object','Object_Type','Confidences'] + iou_thresholds \n",
    "    df_tp = pd.DataFrame(columns=columns)\n",
    "    n_objects,n_objects_s,n_objects_m,n_objects_l = 0,0,0,0\n",
    "\n",
    "    for i,(gt_mask, pred_mask) in enumerate(zip(gt_masks, pred_masks)):\n",
    "\n",
    "        labels_gt = np.unique(gt_mask)\n",
    "        labels_gt = np.delete(labels_gt,np.where(labels_gt == 0))\n",
    "        n_objects += len(labels_gt)\n",
    "        labels_dt = np.unique(pred_mask)\n",
    "        labels_dt = np.delete(labels_dt,np.where(labels_dt == 0))\n",
    "\n",
    "        iou_thresholds = np.arange(0.5, 1.0, 0.05)\n",
    "\n",
    "        '''\n",
    "        if len(labels_gt) != len(labels_dt): \n",
    "            print(f'Frame {i}, GT {len(labels_gt)}, DT{len(labels_dt)}')\n",
    "            print('Ground_truth')\n",
    "            plt.imshow(gt_mask)\n",
    "            plt.show()\n",
    "            print('Prediction')\n",
    "            plt.imshow(pred_mask)\n",
    "            plt.show()\n",
    "        '''\n",
    "        #print(f'Labels GT {labels_gt}')\n",
    "        #print(f'Labels DT {labels_dt}')\n",
    "\n",
    "        #print(f'Scores {scores[i]}')\n",
    "\n",
    "        for j,label in enumerate(labels_gt):\n",
    "            gt_mask_label = gt_mask == label\n",
    "            object_type = None\n",
    "            area = cv2.countNonZero(gt_mask_label*1)\n",
    "            #print(f'Frame {i}, Object {j}')\n",
    "            if area <= 32*32: \n",
    "                object_type = 0\n",
    "                n_objects_s += 1\n",
    "            elif area <= 96*96: \n",
    "                object_type = 1\n",
    "                n_objects_m += 1\n",
    "            else: \n",
    "                object_type = 2\n",
    "                n_objects_l += 1\n",
    "\n",
    "            if label in labels_dt:\n",
    "                pred_mask_label = pred_mask == label\n",
    "\n",
    "                iou = compute_iou(gt_mask_label, pred_mask_label)\n",
    "                tp_values = [1 if iou >= iou_threshold else 0 for iou_threshold in iou_thresholds]\n",
    "                df_tp.loc[len(df_tp)] = np.array([i + 1,label,object_type,scores[i][np.where(labels_dt == label)[0].item()]] + tp_values )\n",
    "\n",
    "    return df_tp,[n_objects,n_objects_s,n_objects_m,n_objects_l]\n",
    "\n",
    "\n",
    "def compute_AP_for_df(df,n):\n",
    "    df = df.sort_values(by='Confidences', ascending=False)\n",
    "    AP = {}\n",
    "    for column in df.columns[4:].tolist():\n",
    "        pos = df.columns.get_loc(column)\n",
    "        df.insert(pos + 1,f'{column}_fp',1 - df[column])\n",
    "        df.insert(pos + 2,f'{column}_accTp',df[column].cumsum())\n",
    "        df.insert(pos + 3,f'{column}_accFp',df[f'{column}_fp'].cumsum())\n",
    "        df.insert(pos + 4,f'{column}_Precision',df[f'{column}_accTp'] / (df[f'{column}_accTp']+df[f'{column}_accFp']))\n",
    "        df.insert(pos + 4,f'{column}_Recall',df[f'{column}_accTp'] / n)\n",
    "        AP[column] = compute_auc_interpolated(df[f'{column}_Precision'].values,df[f'{column}_Recall'].values)\n",
    "        #print_curve(df[f'{column}_Precision'].values,df[f'{column}_Recall'].values,f'Precision Recall Curve_{column}')\n",
    "        #print(AP[column])\n",
    "        df.drop(columns = [f'{column}_fp',f'{column}_accTp',f'{column}_accFp',f'{column}_Precision',f'{column}_Recall'])\n",
    "    return AP\n",
    "\n",
    "\n",
    "def calculate_video_AP(gt_masks, pred_masks, scores):\n",
    "    df,object_counts = true_positives(gt_masks, pred_masks,scores)\n",
    "    AP = compute_AP_for_df(df,object_counts[0])\n",
    "    AP_size = []\n",
    "    for object_type in range(0,3):\n",
    "        if df['Object_Type'].isin([object_type]).any(): AP_size.append(compute_AP_for_df(df[df['Object_Type'] == object_type],[object_counts[object_type+1]]))\n",
    "        else: AP_size.append(None)\n",
    "    return AP, AP_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_davis_set(name, model,videoLoader, compute_metrics = False,save_masks = False, compute_video = False, verbose = True):\n",
    "    df_whole_metrics,df_per_frame_metrics,df_score = None,None,None\n",
    "\n",
    "\n",
    "    if compute_metrics: \n",
    "        g_measures_by_video = ['Video','J&F-Mean', 'J-Mean', 'J-Recall', 'J-Decay', 'F-Mean', 'F-Recall', 'F-Decay','AP','AP@.5','AP@.75','AP_s','AP_m','AP_l','J-Statiscts-Object','F-Statiscts-Object']\n",
    "        df_whole_metrics = pd.DataFrame(columns=g_measures_by_video)\n",
    "        df_score = pd.DataFrame(columns=['Video','Scores'])\n",
    "        df_per_frame_metrics_col = ['Video','Frame','J-Mean','F-Mean','J-Mean-Object','F-Mean-Object','J-Object','F-Object']\n",
    "        df_per_frame_metrics = pd.DataFrame(columns=df_per_frame_metrics_col)\n",
    "    \n",
    "    folder_path = f'./result/{name}'\n",
    "    if compute_metrics or compute_video or save_masks: \n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        if compute_video: \n",
    "            path_to_videos = folder_path + '/videos'\n",
    "            if not os.path.exists(path_to_videos): os.makedirs(path_to_videos)\n",
    "\n",
    "    for video in list(videoLoader):\n",
    "\n",
    "        num_objects, info = video\n",
    "        video_name = info['name'] \n",
    "        num_frames = info['num_frames']\n",
    "        if videoLoader.resolution == '480p':\n",
    "            height = info['size_480p'][1]\n",
    "            width = info['size_480p'][0]\n",
    "        else: \n",
    "            if videoLoader.year == '2017':\n",
    "                height = 3840\n",
    "                width = 2026\n",
    "            else:\n",
    "                height = 1920\n",
    "                width = 1080        \n",
    "        if verbose: print(f'Tracking Video {video_name} with dimensions {width}x{height}')\n",
    "        if verbose: print('Loading dataset images and masks')\n",
    "        images,ground_truth_masks = load_all_images_davis(videoLoader,(video_name,num_frames,num_objects.item()))\n",
    "        \n",
    "        if verbose: print('Creating first annotated mask for VOS model')\n",
    "\n",
    "        model.xmem.current_video = video_name\n",
    "\n",
    "        combined_masks = [[mask * (i+1) for i, mask in enumerate(frameMask)] for frameMask in ground_truth_masks]\n",
    "        ground_truth_masks = [unifyMasks(mask, height, width) for mask in combined_masks]\n",
    "        initial_mask = ground_truth_masks[0]\n",
    "        #Compute masks for all images\n",
    "        \n",
    "        if verbose:print('Computing all masks')\n",
    "        model.xmem.clear_memory()\n",
    "        masks, logits, painted_images, scores = model.generator(images=images, template_mask=initial_mask)\n",
    "        model.xmem.clear_memory() \n",
    "        \n",
    "        df_score.loc[len(df_score)] = [video_name, [item[0] for item in scores]]\n",
    "        #df_score.append({'Video': video_name, 'Scores': [item[0] for item in scores]}, ignore_index=True)\n",
    "        \n",
    "        if compute_metrics:\n",
    "            if verbose: print('Computing Metrics')\n",
    "            (f_mean, f_recall, f_decay),(j_mean, j_recall, j_decay),\\\n",
    "            (f_mean_obj, f_recall_obj, f_decay_obj),(j_mean_obj, j_recall_obj, j_decay_obj),\\\n",
    "            f_frame,j_frame = compute_all_video_metrics(video_name,masks[1:],ground_truth_masks[1:],df_per_frame_metrics)\n",
    "\n",
    "            AP, AP_objectSize = calculate_video_AP(ground_truth_masks[1:],masks[1:], scores)\n",
    "\n",
    "            AP_n = sum(AP.values())/len(AP)\n",
    "            AP_5 = AP[0.5]\n",
    "            AP_75 = AP[0.75]\n",
    "            AP_s = sum(AP_objectSize[0].values())/len(AP_objectSize[0]) if AP_objectSize[0] is not None else np.nan\n",
    "            AP_m = sum(AP_objectSize[1].values())/len(AP_objectSize[1]) if AP_objectSize[1] is not None else np.nan\n",
    "            AP_l = sum(AP_objectSize[2].values())/len(AP_objectSize[2]) if AP_objectSize[2] is not None else np.nan\n",
    "\n",
    "            df_whole_metrics.loc[len(df_whole_metrics)] = np.array([video_name,(f_mean+j_mean)/2,j_mean,j_recall,j_decay,f_mean,f_recall,f_decay,AP_n,AP_5,AP_75,AP_s,AP_m,AP_l,j_frame,f_frame])\n",
    "            df_whole_metrics.loc[len(df_whole_metrics)] = np.array([video_name + '_object',(f_mean_obj+j_mean_obj)/2,j_mean_obj,j_recall_obj,j_decay_obj,f_mean_obj,f_recall_obj,f_decay_obj,AP_n,AP_5,AP_75,AP_s,AP_m,AP_l,j_frame,f_frame])\n",
    "\n",
    "         \n",
    "        if compute_video: \n",
    "            if verbose: print('Generating video')\n",
    "            if width % 2 != 0 or height % 2 != 0: \n",
    "                painted_images = pad_to_divisible_by_two(painted_images)\n",
    "            generate_video_from_frames(painted_images, output_path= path_to_videos + f\"/{video_name}.mp4\", fps = 10) \n",
    "\n",
    "        if save_masks:\n",
    "            if verbose: print('Saving masks') \n",
    "            path_to_masks = folder_path + '/masks/' + video_name\n",
    "            if not os.path.exists(path_to_masks): os.makedirs(path_to_masks)\n",
    "            for i,mask in enumerate(painted_images): \n",
    "                image = Image.fromarray(mask)\n",
    "                image.save(os.path.join(path_to_masks, '{:05d}.png'.format(i)))\n",
    "    \n",
    "    if compute_metrics:\n",
    "        df_per_frame_metrics.to_csv(folder_path + '/per_object_metrics.csv',index=False)\n",
    "        df_whole_metrics.to_csv(folder_path + '/whole_metrics.csv',index=False)\n",
    "\n",
    "        all_test_metrics = None\n",
    "        if os.path.exists(all_tests_csv):\n",
    "            all_test_metrics = pd.read_csv(all_tests_csv,index_col = None)\n",
    "        else:\n",
    "            all_test_col = ['Test','J&F-Mean', 'J-Mean', 'J-Recall', 'J-Decay', 'F-Mean', 'F-Recall', 'F-Decay',\\\n",
    "                                    'AP-Mean','AP@.5-Mean','AP@.75-Mean','AP_s-Mean','AP_m-Mean','AP_l-Mean',\\\n",
    "                                    'J&F-Mean-Obj', 'J-Mean-Obj', 'J-Recall-Obj', 'J-Decay-Obj', 'F-Mean-Obj', 'F-Recall-Obj','F-Decay-Obj']\n",
    "            all_test_metrics = pd.DataFrame(columns=all_test_col)\n",
    "        normal_mean = df_whole_metrics[~df_whole_metrics['Video'].str.contains('_object')].iloc[:, 1:-8].mean().tolist()\n",
    "        ap_mean = df_whole_metrics[df_whole_metrics['Video'].str.contains('_object')].iloc[:, 8:14].mean().tolist()\n",
    "        per_object_mean = df_whole_metrics[df_whole_metrics['Video'].str.contains('_object')].iloc[:, 1:-8].mean().tolist()\n",
    "        all_test_metrics.loc[len(all_test_metrics)] = np.array([name] + normal_mean + ap_mean + per_object_mean)\n",
    "        all_test_metrics.to_csv(all_tests_csv, index = False)\n",
    "    \n",
    "    df_score.to_csv(folder_path + '/scores.csv',index=False)\n",
    "\n",
    "    return masks, logits, painted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_ovis_set(name, model,path_set,videos, annotations, compute_metrics = False,save_masks = False, compute_video = False, verbose = True):\n",
    "    df_whole_metrics,df_per_frame_metrics,df_score = None,None,None\n",
    "    if compute_metrics: \n",
    "        g_measures_by_video = ['Video','J&F-Mean', 'J-Mean', 'J-Recall', 'J-Decay', 'F-Mean', 'F-Recall', 'F-Decay','AP','AP@.5','AP@.75','AP_s','AP_m','AP_l','J-Statiscts-Object','F-Statiscts-Object']\n",
    "        df_whole_metrics = pd.DataFrame(columns=g_measures_by_video)\n",
    "        df_per_frame_metrics_col = ['Video','Frame','J','F','J-Mean','F-Mean','J-Object','F-Object']\n",
    "        df_per_frame_metrics = pd.DataFrame(columns=df_per_frame_metrics_col)\n",
    "\n",
    "    folder_path = f'./result/{name}'\n",
    "    if compute_metrics or compute_video or save_masks: \n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        if compute_video: \n",
    "            path_to_videos = folder_path + '/videos'\n",
    "            if not os.path.exists(path_to_videos): os.makedirs(path_to_videos)\n",
    "\n",
    "    for video in videos:\n",
    "        # Load all images as np.array\n",
    "        if verbose: print(f'max memory allocated: {torch.cuda.max_memory_allocated()/(2**20)} MB')\n",
    "        video_folder = video[\"file_names\"][0].split(\"/\")[0]\n",
    "        model.xmem.current_video = video_folder\n",
    "        if verbose: print(f'Tracking Video {video_folder} with dimensions {video[\"width\"]}x{video[\"height\"]}')\n",
    "        if verbose: print('Loading dataset images')\n",
    "        images = load_images_from_folder(path_set,video['file_names'])\n",
    "\n",
    "        # Load al poligon of first image to a usable mask\n",
    "        if verbose: print('Creating first annotated mask for VOS model')\n",
    "        ann = [a for a in annotations if a['video_id'] == video['id']]\n",
    "        masks = [(annToMask(a, 0) * (i + 1))  for i, a in enumerate(ann) if annToMask(a, 0) is not None ]\n",
    "        number_masks = len(masks)\n",
    "        if verbose: print(f'Initial loaded MASKS {number_masks}')\n",
    "        initial_mask = unifyMasks(masks, video['width'], video['height'])\n",
    "\n",
    "\n",
    "        #Compute masks for all images\n",
    "        if verbose:print('Computing all masks')\n",
    "        model.xmem.clear_memory()\n",
    "        masks, logits, painted_images,scores = model.generator(images=images, template_mask=initial_mask)\n",
    "        model.xmem.clear_memory()  \n",
    "\n",
    "        \n",
    "        if compute_metrics:\n",
    "            if verbose: print('Computing Metrics')\n",
    "            ground_truth_masks = load_all_masks_for_video(video,number_masks)\n",
    "            (f_mean, f_recall, f_decay),(j_mean, j_recall, j_decay),\\\n",
    "            (f_mean_obj, f_recall_obj, f_decay_obj),(j_mean_obj, j_recall_obj, j_decay_obj),\\\n",
    "            f_frame,j_frame = compute_all_video_metrics(video_folder,masks[1:],ground_truth_masks[1:],df_per_frame_metrics)\n",
    "\n",
    "            AP, AP_objectSize = calculate_video_AP(ground_truth_masks[1:],masks[1:], scores)\n",
    "\n",
    "            AP_n = sum(AP.values())/len(AP)\n",
    "            AP_5 = AP[0.5]\n",
    "            AP_75 = AP[0.75]\n",
    "            AP_s = sum(AP_objectSize[0].values())/len(AP_objectSize[0]) if AP_objectSize[0] is not None else np.nan\n",
    "            AP_m = sum(AP_objectSize[1].values())/len(AP_objectSize[1]) if AP_objectSize[1] is not None else np.nan\n",
    "            AP_l = sum(AP_objectSize[2].values())/len(AP_objectSize[2]) if AP_objectSize[2] is not None else np.nan\n",
    "\n",
    "            df_whole_metrics.loc[len(df_whole_metrics)] = np.array([video_folder,(f_mean+j_mean)/2,j_mean,j_recall,j_decay,f_mean,f_recall,f_decay,AP_n,AP_5,AP_75,AP_s,AP_m,AP_l,j_frame,f_frame])\n",
    "            df_whole_metrics.loc[len(df_whole_metrics)] = np.array([video_folder + '_object',(f_mean_obj+j_mean_obj)/2,j_mean_obj,j_recall_obj,j_decay_obj,f_mean_obj,f_recall_obj,f_decay_obj,AP_n,AP_5,AP_75,AP_s,AP_m,AP_l,j_frame,f_frame])\n",
    "\n",
    "                \n",
    "        if compute_video: \n",
    "            if verbose: print('Generating video')\n",
    "            if video['width'] % 2 != 0 or video['height'] % 2 != 0: \n",
    "                painted_images = pad_to_divisible_by_two(painted_images)\n",
    "            generate_video_from_frames(painted_images, output_path= path_to_videos + f\"/{video['id']}_{video_folder}.mp4\", fps = 10) \n",
    "\n",
    "        if save_masks:\n",
    "            if verbose: print('Saving masks') \n",
    "            path_to_masks = folder_path + '/masks/' + video_folder\n",
    "            if not os.path.exists(path_to_masks): os.makedirs(path_to_masks)\n",
    "            for i,mask in enumerate(painted_images): \n",
    "                image = Image.fromarray(mask)\n",
    "                image.save(os.path.join(path_to_masks, '{:05d}.png'.format(i + 1)))\n",
    "    \n",
    "    if compute_metrics:\n",
    "        df_per_frame_metrics.to_csv(folder_path + '/per_object_metrics.csv',index=False)\n",
    "        df_whole_metrics.to_csv(folder_path + '/whole_metrics.csv',index=False)\n",
    "\n",
    "        all_test_metrics = None\n",
    "        if os.path.exists(all_tests_csv):\n",
    "            all_test_metrics = pd.read_csv(all_tests_csv,index_col = None)\n",
    "        else:\n",
    "            all_test_col = ['Test','J&F-Mean', 'J-Mean', 'J-Recall', 'J-Decay', 'F-Mean', 'F-Recall', 'F-Decay',\\\n",
    "                                    'AP-Mean','AP@.5-Mean','AP@.75-Mean','AP_s-Mean','AP_m-Mean','AP_l-Mean',\\\n",
    "                                    'J&F-Mean-Obj', 'J-Mean-Obj', 'J-Recall-Obj', 'J-Decay-Obj', 'F-Mean-Obj', 'F-Recall-Obj','F-Decay-Obj']\n",
    "            all_test_metrics = pd.DataFrame(columns=all_test_col)\n",
    "        normal_mean = df_whole_metrics[~df_whole_metrics['Video'].str.contains('_object')].iloc[:, 1:-8].mean().tolist()\n",
    "        ap_mean = df_whole_metrics[df_whole_metrics['Video'].str.contains('_object')].iloc[:, 8:14].mean().tolist()\n",
    "        per_object_mean = df_whole_metrics[df_whole_metrics['Video'].str.contains('_object')].iloc[:, 1:-8].mean().tolist()\n",
    "        all_test_metrics.loc[len(all_test_metrics)] = np.array([name] + normal_mean + ap_mean + per_object_mean)\n",
    "        all_test_metrics.to_csv(all_tests_csv, index = False)\n",
    "                \n",
    "    return masks, logits, painted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_longdata_set(name, model,videoLoader, compute_metrics = False,save_masks = False, compute_video = False, verbose = True):\n",
    "    df_whole_metrics,df_per_frame_metrics = None,None\n",
    "    if compute_metrics: \n",
    "        g_measures_by_video = ['Video','J&F-Mean', 'J-Mean', 'J-Recall', 'J-Decay', 'F-Mean', 'F-Recall', 'F-Decay','AP','AP@.5','AP@.75','AP_s','AP_m','AP_l','J-Statiscts-Object','F-Statiscts-Object']\n",
    "        df_whole_metrics = pd.DataFrame(columns=g_measures_by_video)\n",
    "        df_per_frame_metrics_col = ['Video','Frame','J-Mean','F-Mean','J-Mean-Object','F-Mean-Object','J-Object','F-Object']\n",
    "        df_per_frame_metrics = pd.DataFrame(columns=df_per_frame_metrics_col)\n",
    "    \n",
    "    folder_path = f'./result/{name}'\n",
    "    if compute_metrics or compute_video or save_masks: \n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        if compute_video: \n",
    "            path_to_videos = folder_path + '/videos'\n",
    "            if not os.path.exists(path_to_videos): os.makedirs(path_to_videos)\n",
    "    \n",
    "    for seq in list(videoLoader.get_sequences()):\n",
    "        \n",
    "        model.xmem.current_video = seq\n",
    "        if verbose: print(f'Video: {seq}')\n",
    "\n",
    "        all_gt_masks, _, all_masks_id = videoLoader.get_all_masks(seq, True)\n",
    "        if os.name == 'nt': all_masks_id = [int(folder.split('\\\\')[-1]) for folder in all_masks_id]\n",
    "        images_root = os.path.join(videoLoader.root_folder ,'JPEGImages',seq)\n",
    "        file_names = sorted(os.listdir(images_root))\n",
    "        file_ids = [int(name.split('.')[0]) for name in file_names]\n",
    "        test_ids = [file_ids.index(int(mask_id)) for mask_id in all_masks_id[1:]]\n",
    "        all_frames = load_images_from_folder(images_root,file_names)\n",
    "        initial_mask = all_gt_masks[0,0,:,:]\n",
    "        #test_ids = [int((int(all_masks_id[i]) - int(all_masks_id[0]))/3) for i in range(1,len(all_masks_id))]\n",
    "        width,height,_= all_frames[0].shape\n",
    "        \n",
    "        #Compute masks for all images\n",
    "        if verbose:print('Computing all masks')\n",
    "        model.xmem.clear_memory()\n",
    "        masks, logits, painted_images, scores = model.generator(images=all_frames, template_mask=initial_mask)\n",
    "        model.xmem.clear_memory()  \n",
    "        \n",
    "        if compute_metrics:\n",
    "            if verbose: print('Computing Metrics')\n",
    "            masks_compute =  [masks[i] for i in test_ids]\n",
    "            (f_mean, f_recall, f_decay),(j_mean, j_recall, j_decay),\\\n",
    "            (f_mean_obj, f_recall_obj, f_decay_obj),(j_mean_obj, j_recall_obj, j_decay_obj),\\\n",
    "            f_frame,j_frame = compute_all_video_metrics(seq,masks_compute,all_gt_masks[0,1:,:,:],df_per_frame_metrics)\n",
    "            \n",
    "            AP, AP_objectSize = calculate_video_AP(all_gt_masks[0,1:,:,:],masks_compute, scores)\n",
    "\n",
    "            AP_n = sum(AP.values())/len(AP)\n",
    "            AP_5 = AP[0.5]\n",
    "            AP_75 = AP[0.75]\n",
    "            AP_s = sum(AP_objectSize[0].values())/len(AP_objectSize[0]) if AP_objectSize[0] is not None else np.nan\n",
    "            AP_m = sum(AP_objectSize[1].values())/len(AP_objectSize[1]) if AP_objectSize[1] is not None else np.nan\n",
    "            AP_l = sum(AP_objectSize[2].values())/len(AP_objectSize[2]) if AP_objectSize[2] is not None else np.nan\n",
    "\n",
    "            df_whole_metrics.loc[len(df_whole_metrics)] = np.array([seq,(f_mean+j_mean)/2,j_mean,j_recall,j_decay,f_mean,f_recall,f_decay,AP_n,AP_5,AP_75,AP_s,AP_m,AP_l,j_frame,f_frame])\n",
    "            df_whole_metrics.loc[len(df_whole_metrics)] = np.array([seq + '_object',(f_mean_obj+j_mean_obj)/2,j_mean_obj,j_recall_obj,j_decay_obj,f_mean_obj,f_recall_obj,f_decay_obj,AP_n,AP_5,AP_75,AP_s,AP_m,AP_l,j_frame,f_frame])\n",
    "\n",
    "                \n",
    "        if compute_video: \n",
    "            if verbose: print('Generating video')\n",
    "            if width % 2 != 0 or height % 2 != 0: \n",
    "                painted_images = pad_to_divisible_by_two(painted_images)\n",
    "            generate_video_from_frames(painted_images, output_path= path_to_videos + f\"/{seq}.mp4\", fps = 10) \n",
    "\n",
    "        if save_masks:\n",
    "            if verbose: print('Saving masks') \n",
    "            path_to_masks = folder_path + '/masks/' + seq\n",
    "            if not os.path.exists(path_to_masks): os.makedirs(path_to_masks)\n",
    "            #for i,mask in enumerate(painted_images): \n",
    "            #    image = Image.fromarray(mask)\n",
    "            #    image.save(os.path.join(path_to_masks, '{:05d}.png'.format(i)))\n",
    "            path_to_masks = path_to_masks + '/testedmasks'\n",
    "            if not os.path.exists(path_to_masks): os.makedirs(path_to_masks)\n",
    "            for i,j in zip(test_ids,all_masks_id[1:]): \n",
    "                image = Image.fromarray(painted_images[i])\n",
    "                image.save(os.path.join(path_to_masks, '{:05d}.png'.format(j)))\n",
    "            \n",
    "                \n",
    "    if compute_metrics:\n",
    "        df_per_frame_metrics.to_csv(folder_path + '/per_object_metrics.csv',index=False)\n",
    "        df_whole_metrics.to_csv(folder_path + '/whole_metrics.csv',index=False)\n",
    "\n",
    "        all_test_metrics = None\n",
    "        if os.path.exists(all_tests_csv):\n",
    "            all_test_metrics = pd.read_csv(all_tests_csv,index_col = None)\n",
    "        else:\n",
    "            all_test_col = ['Test','J&F-Mean', 'J-Mean', 'J-Recall', 'J-Decay', 'F-Mean', 'F-Recall', 'F-Decay',\\\n",
    "                                    'AP-Mean','AP@.5-Mean','AP@.75-Mean','AP_s-Mean','AP_m-Mean','AP_l-Mean',\\\n",
    "                                    'J&F-Mean-Obj', 'J-Mean-Obj', 'J-Recall-Obj', 'J-Decay-Obj', 'F-Mean-Obj', 'F-Recall-Obj','F-Decay-Obj']\n",
    "            all_test_metrics = pd.DataFrame(columns=all_test_col)\n",
    "        normal_mean = df_whole_metrics[~df_whole_metrics['Video'].str.contains('_object')].iloc[:, 1:-8].mean().tolist()\n",
    "        ap_mean = df_whole_metrics[df_whole_metrics['Video'].str.contains('_object')].iloc[:, 8:14].mean().tolist()\n",
    "        per_object_mean = df_whole_metrics[df_whole_metrics['Video'].str.contains('_object')].iloc[:, 1:-8].mean().tolist()\n",
    "        all_test_metrics.loc[len(all_test_metrics)] = np.array([name] + normal_mean + ap_mean + per_object_mean)\n",
    "        all_test_metrics.to_csv(all_tests_csv, index = False)\n",
    "                \n",
    "    return masks, logits, painted_images\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM_checkpoint = \"./checkpoints/sam_vit_h_4b8939.pth\"\n",
    "if 'HQ' in sys.prefix.split('.')[-1]:SAM_checkpoint = \"./checkpoints/sam_hq_vit_h.pth\"\n",
    "xmem_checkpoint = \"./checkpoints/XMem-s012.pth\"\n",
    "#e2fgvi_checkpoint = \"./checkpoints/E2FGVI-HQ-CVPR22.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x2598005c750>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeargs_lst = [ \n",
    "{\n",
    "    'DatasetArgs' :{\n",
    "        'Dataset' : 'Ovis'\n",
    "    },\n",
    "    'TrackingAnythingArgs' : {\n",
    "            'use_refinement' : True,\n",
    "            'refinement_mode' : 'mask_bbox_pos_neg',\n",
    "            'addArgs1':'C&Poly'\n",
    "    }\n",
    "}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Gets most ocludded smallest videos in the dataset'''\n",
    "from collections import Counter\n",
    "columns = ['video_id','no_occlusion','slight_occlusion','severe_occlusion']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "for ann in annTrain:\n",
    "    counted_repetitions = Counter(ann['occlusion'])\n",
    "    for key in columns[1:]:\n",
    "        counted_repetitions[key] = counted_repetitions.get(key, 0)\n",
    "    if ann['video_id'] in df.index: \n",
    "        df.loc[ann['video_id'],'no_occlusion'] += counted_repetitions['no_occlusion']\n",
    "        df.loc[ann['video_id'],'slight_occlusion'] += counted_repetitions['slight_occlusion']\n",
    "        df.loc[ann['video_id'],'severe_occlusion'] += counted_repetitions['severe_occlusion']\n",
    "    else: df.loc[ann['video_id']] = np.array([ann['video_id'],counted_repetitions['no_occlusion'],counted_repetitions['slight_occlusion'],counted_repetitions['severe_occlusion']])\n",
    "df['occlusion_value'] = (df['slight_occlusion'] + df['severe_occlusion'] * 1.5 )/(df['no_occlusion'] + df['slight_occlusion'] + df['severe_occlusion']* 1.5)\n",
    "df['total_masks'] = (df['no_occlusion'] + df['slight_occlusion'] + df['severe_occlusion'])\n",
    "df['frames'] = [len(vid['file_names']) for vid in vidTrain]\n",
    "df['width'] = [vid['width'] for vid in vidTrain]\n",
    "df['height'] = [vid['height'] for vid in vidTrain]\n",
    "df['size'] = [vid['height']*vid['height']*len(vid['file_names']) for vid in vidTrain]\n",
    "listed = df.sort_values(by=['occlusion_value', 'size'], ascending=[False,True]).head(54)['video_id'].values\n",
    "occludedTrain = [d for id_ in listed for d in vidTrain if d.get('id') == id_]\n",
    "remove = [18,19,38,43]\n",
    "occludedTrain  = [item for i, item in enumerate(occludedTrain) if i not in remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop OVIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for runtimeargs in runtimeargs_lst:\n",
    "    #vidToTest = select_smaller_set(vidTrain,25)\n",
    "    runname = '{}{}{}{}_{}_{}'.format(\n",
    "        'Refined' if runtimeargs['TrackingAnythingArgs']['use_refinement']  else 'XMEM',\n",
    "        '_HQ' if 'HQ' in sys.prefix.split('.')[-1] else '_SAM',\n",
    "        '_'+ runtimeargs['TrackingAnythingArgs']['refinement_mode'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] else '',\n",
    "        '_' + runtimeargs['TrackingAnythingArgs']['addArgs1'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] and runtimeargs['TrackingAnythingArgs']['addArgs1'] != '' else '',\n",
    "        'Davis_{}_{}_{}'.format(runtimeargs['DatasetArgs']['Year'],runtimeargs['DatasetArgs']['Set'],runtimeargs['DatasetArgs']['Resolution']) if runtimeargs['DatasetArgs']['Dataset'] == 'Davis' else runtimeargs['DatasetArgs']['Dataset'],\n",
    "        ''.join(random.choice(string.ascii_letters) for _ in range(5))\n",
    "    )\n",
    "    print(f'Running Test: {runname}')\n",
    "    model = TrackingAnything(SAM_checkpoint, xmem_checkpoint, None,runtimeargs['TrackingAnythingArgs'], save_inner_masks_folder = runname)\n",
    "    masks, logits, painted_images = run_model_on_ovis_set(name = runname,model = model, path_set = ovis_images,videos = occludedTrain,annotations = annTrain,compute_metrics = True, save_masks=True, compute_video=True,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = runtimeargs_lst[0]['DatasetArgs']['Set']+'.txt'\n",
    "VideoLoader = LongVideoDataset(root_folder=longdataset_root, filelist= filelist)\n",
    "xmem_checkpoint = 'checkpoints\\XMem.pth'\n",
    "\n",
    "for runtimeargs in runtimeargs_lst:\n",
    "    runname = '{}{}{}{}_{}_{}'.format(\n",
    "    'Refined' if runtimeargs['TrackingAnythingArgs']['use_refinement']  else 'XMEM',\n",
    "    '_HQ' if 'HQ' in sys.prefix.split('.')[-1] else '_SAM',\n",
    "    '_'+ runtimeargs['TrackingAnythingArgs']['refinement_mode'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] else '',\n",
    "    '_' + runtimeargs['TrackingAnythingArgs']['addArgs1'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] and runtimeargs['TrackingAnythingArgs']['addArgs1'] != '' else '',\n",
    "    'Davis_{}_{}_{}'.format(runtimeargs['DatasetArgs']['Year'],runtimeargs['DatasetArgs']['Set'],runtimeargs['DatasetArgs']['Resolution']) if runtimeargs['DatasetArgs']['Dataset'] == 'Davis' else runtimeargs['DatasetArgs']['Dataset'],\n",
    "    ''.join(random.choice(string.ascii_letters) for _ in range(5))\n",
    "    )\n",
    "    print(f'Running Test: {runname}')\n",
    "    model = TrackingAnything(SAM_checkpoint, xmem_checkpoint, None,runtimeargs['TrackingAnythingArgs'], save_inner_masks_folder = runname)\n",
    "    masks, logits, painted_images = run_model_on_longdata_set(name = runname,model = model,videoLoader = VideoLoader,compute_metrics = True, save_masks=True, compute_video=True,verbose = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop Davis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = runtimeargs_lst[0]['DatasetArgs']['Resolution']\n",
    "year = runtimeargs_lst[0]['DatasetArgs']['Year']\n",
    "data_set = runtimeargs_lst[0]['DatasetArgs']['Set']\n",
    "VideoLoader = DAVIS_MO_Test(davis_root, resolution=resolution, imset='20{}/{}.txt'.format(year,data_set), single_object=(year==16))\n",
    "\n",
    "for runtimeargs in runtimeargs_lst:\n",
    "    runname = '{}{}{}{}_{}_{}'.format(\n",
    "    'Refined' if runtimeargs['TrackingAnythingArgs']['use_refinement']  else 'XMEM',\n",
    "    '_HQ' if 'HQ' in sys.prefix.split('.')[-1] else '_SAM',\n",
    "    '_'+ runtimeargs['TrackingAnythingArgs']['refinement_mode'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] else '',\n",
    "    '_' + runtimeargs['TrackingAnythingArgs']['addArgs1'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] and runtimeargs['TrackingAnythingArgs']['addArgs1'] != '' else '',\n",
    "    'Davis_{}_{}_{}'.format(runtimeargs['DatasetArgs']['Year'],runtimeargs['DatasetArgs']['Set'],runtimeargs['DatasetArgs']['Resolution']) if runtimeargs['DatasetArgs']['Dataset'] == 'Davis' else runtimeargs['DatasetArgs']['Dataset'],\n",
    "    ''.join(random.choice(string.ascii_letters) for _ in range(5))\n",
    "    )\n",
    "    print(f'Running Test: {runname}')\n",
    "    model = TrackingAnything(SAM_checkpoint, xmem_checkpoint, None,runtimeargs['TrackingAnythingArgs'], save_inner_masks_folder = runname)\n",
    "    masks, logits, painted_images = run_model_on_davis_set(name = runname,model = model,videoLoader = VideoLoader,compute_metrics = True, save_masks=True, compute_video=True,verbose = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAVIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeargs = {\n",
    "    'DatasetArgs' :{\n",
    "        'Dataset' : 'Davis',\n",
    "        'Year' : 16,\n",
    "        'Set' : 'val',\n",
    "        'Resolution': '480p'\n",
    "    },\n",
    "    'TrackingAnythingArgs' : {\n",
    "            'use_refinement' : True,\n",
    "            'refinement_mode' : 'point',\n",
    "            'addArgs1':''\n",
    "    }\n",
    "}\n",
    "\n",
    "VideoLoader = DAVIS_MO_Test(davis_root, resolution=runtimeargs['DatasetArgs']['Resolution'], imset='20{}/{}.txt'.format(runtimeargs['DatasetArgs']['Year'],runtimeargs['DatasetArgs']['Set']), single_object=(runtimeargs['DatasetArgs']['Year']==16))\n",
    "runname = '{}{}{}{}_{}_{}'.format(\n",
    "    'Refined' if runtimeargs['TrackingAnythingArgs']['use_refinement']  else 'XMEM',\n",
    "    '_HQ' if 'HQ' in sys.prefix.split('.')[-1] else '_SAM',\n",
    "    '_'+ runtimeargs['TrackingAnythingArgs']['refinement_mode'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] else '',\n",
    "    '_' + runtimeargs['TrackingAnythingArgs']['addArgs1'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] and runtimeargs['TrackingAnythingArgs']['addArgs1'] != '' else '',\n",
    "    'Davis_{}_{}_{}'.format(runtimeargs['DatasetArgs']['Year'],runtimeargs['DatasetArgs']['Set'],runtimeargs['DatasetArgs']['Resolution']) if runtimeargs['DatasetArgs']['Dataset'] == 'Davis' else runtimeargs['DatasetArgs']['Dataset'],\n",
    "    ''.join(random.choice(string.ascii_letters) for _ in range(5))\n",
    ")\n",
    "print(f'Running Test: {runname}')\n",
    "model = TrackingAnything(SAM_checkpoint, xmem_checkpoint, None,runtimeargs['TrackingAnythingArgs'], save_inner_masks_folder = runname)\n",
    "masks, logits, painted_images = run_model_on_davis_set(name = runname,model = model,videoLoader = VideoLoader,compute_metrics = True, save_masks=True, compute_video=True,verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LONG DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeargs = {\n",
    "    'DatasetArgs' :{\n",
    "        'Dataset' : 'LongDataset',\n",
    "        'Set' : 'val',\n",
    "    },\n",
    "    'TrackingAnythingArgs' : {\n",
    "            'use_refinement' : False\n",
    "                }\n",
    "}\n",
    "\n",
    "VideoLoader = LongVideoDataset(root_folder=longdataset_root, filelist= runtimeargs['DatasetArgs']['Set']+'.txt')\n",
    "runname = '{}{}{}{}_{}_{}'.format(\n",
    "    'Refined' if runtimeargs['TrackingAnythingArgs']['use_refinement']  else 'XMEM',\n",
    "    '_HQ' if 'HQ' in sys.prefix.split('.')[-1] else '_SAM',\n",
    "    '_'+ runtimeargs['TrackingAnythingArgs']['refinement_mode'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] else '',\n",
    "    '_' + runtimeargs['TrackingAnythingArgs']['addArgs1'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] and runtimeargs['TrackingAnythingArgs']['addArgs1'] != '' else '',\n",
    "    'Davis_{}_{}_{}'.format(runtimeargs['DatasetArgs']['Year'],runtimeargs['DatasetArgs']['Set'],runtimeargs['DatasetArgs']['Resolution']) if runtimeargs['DatasetArgs']['Dataset'] == 'Davis' else runtimeargs['DatasetArgs']['Dataset'],\n",
    "    ''.join(random.choice(string.ascii_letters) for _ in range(5))\n",
    ")\n",
    "print(f'Running Test: {runname}')\n",
    "xmem_checkpoint = 'checkpoints\\XMem.pth'\n",
    "model = TrackingAnything(SAM_checkpoint, xmem_checkpoint, None,runtimeargs['TrackingAnythingArgs'], save_inner_masks_folder = runname)\n",
    "masks, logits, painted_images = run_model_on_longdata_set(name = runname,model = model,videoLoader = VideoLoader,compute_metrics = True, save_masks=False, compute_video=False,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OVIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeargs = {\n",
    "    'DatasetArgs' :{\n",
    "        'Dataset' : 'Ovis'\n",
    "    },\n",
    "    'TrackingAnythingArgs' : {\n",
    "            'use_refinement' : False,\n",
    "            'refinement_mode' : 'bbox',\n",
    "            'addArgs1':''\n",
    "    }\n",
    "}\n",
    "\n",
    "vidToTest = select_smaller_set(vidTrain,25)\n",
    "runname = '{}{}{}{}_{}_{}'.format(\n",
    "    'Refined' if runtimeargs['TrackingAnythingArgs']['use_refinement']  else 'XMEM',\n",
    "    '_HQ' if 'HQ' in sys.prefix.split('.')[-1] else '_SAM',\n",
    "    '_'+ runtimeargs['TrackingAnythingArgs']['refinement_mode'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] else '',\n",
    "    '_' + runtimeargs['TrackingAnythingArgs']['addArgs1'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] and runtimeargs['TrackingAnythingArgs']['addArgs1'] != '' else '',\n",
    "    'Davis_{}_{}_{}'.format(runtimeargs['DatasetArgs']['Year'],runtimeargs['DatasetArgs']['Set'],runtimeargs['DatasetArgs']['Resolution']) if runtimeargs['DatasetArgs']['Dataset'] == 'Davis' else runtimeargs['DatasetArgs']['Dataset'],\n",
    "    ''.join(random.choice(string.ascii_letters) for _ in range(5))\n",
    ")\n",
    "print(f'Running Test: {runname}')\n",
    "model = TrackingAnything(SAM_checkpoint, xmem_checkpoint, None,runtimeargs['TrackingAnythingArgs'], save_inner_masks_folder = runname)\n",
    "masks, logits, painted_images = run_model_on_ovis_set(name = runname,model = model, path_set = ovis_images,videos = vidToTest,annotations = annTrain,compute_metrics = True, save_masks=True, compute_video=True,verbose = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUNK TESTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_longVOS_set(name, model,videoLoader, compute_metrics = False,save_masks = False, compute_video = False, verbose = True):\n",
    "    \n",
    "    folder_path = f'./resultLongVOS/{name}'\n",
    "    if compute_metrics or compute_video or save_masks: \n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        if compute_video: \n",
    "            path_to_videos = folder_path + '/videos'\n",
    "            if not os.path.exists(path_to_videos): os.makedirs(path_to_videos)\n",
    "    \n",
    "    for seq in list(videoLoader.get_sequences()):\n",
    "        \n",
    "        model.xmem.current_video = seq\n",
    "        if verbose: print(f'Video: {seq}')\n",
    "\n",
    "        all_gt_masks, _, all_masks_id = videoLoader.get_all_masks(seq, True) \n",
    "        if os.name == 'nt': all_masks_id = [int(folder.split('\\\\')[-1]) for folder in all_masks_id]\n",
    "        images_root = os.path.join(videoLoader.root_folder ,'JPEGImages',seq)\n",
    "        file_names = sorted(os.listdir(images_root))\n",
    "        file_ids = [int(name.split('.')[0]) for name in file_names]\n",
    "        test_ids = [file_ids.index(int(mask_id)) for mask_id in all_masks_id[1:]]\n",
    "        all_frames = load_images_from_folder(images_root,file_names)\n",
    "        initial_mask = all_gt_masks[0,0,:,:]\n",
    "        #test_ids = [int((int(all_masks_id[i]) - int(all_masks_id[0]))/3) for i in range(1,len(all_masks_id))]\n",
    "        width,height,_= all_frames[0].shape\n",
    "        \n",
    "        #Compute masks for all images\n",
    "        if verbose:print('Computing all masks')\n",
    "        model.xmem.clear_memory()\n",
    "        masks, logits, painted_images, scores = model.generator(images=all_frames, template_mask=initial_mask)\n",
    "        model.xmem.clear_memory()  \n",
    "        \n",
    "        if compute_video: \n",
    "            if verbose: print('Generating video')\n",
    "            if width % 2 != 0 or height % 2 != 0: \n",
    "                painted_images = pad_to_divisible_by_two(painted_images)\n",
    "            generate_video_from_frames(painted_images, output_path= path_to_videos + f\"/{seq}.mp4\", fps = 10) \n",
    "\n",
    "        if save_masks:\n",
    "            if verbose: print('Saving masks') \n",
    "            path_to_masks = folder_path + '/masks/' + seq\n",
    "            if not os.path.exists(path_to_masks): os.makedirs(path_to_masks)\n",
    "            #for i,mask in enumerate(painted_images): \n",
    "            #    image = Image.fromarray(mask)\n",
    "            #    image.save(os.path.join(path_to_masks, '{:05d}.png'.format(i)))\n",
    "            path_to_masks = path_to_masks + '/testedmasks'\n",
    "            if not os.path.exists(path_to_masks): os.makedirs(path_to_masks)\n",
    "            for i,j in zip(test_ids,all_masks_id[1:]): \n",
    "                image = Image.fromarray(painted_images[i])\n",
    "                image.save(os.path.join(path_to_masks, '{:05d}.png'.format(j)))\n",
    "\n",
    "    return masks, logits, painted_images\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeargs_lst = [ \n",
    "    {\n",
    "    'DatasetArgs' :{\n",
    "        'Dataset' : 'LongVOS',\n",
    "        'Set' : 'test',\n",
    "    },\n",
    "    'TrackingAnythingArgs' : {\n",
    "            'use_refinement' : True,\n",
    "            'refinement_mode' : 'mask_bbox_pos_neg',\n",
    "            'addArgs1':'C&Poly'\n",
    "    }\n",
    "},\n",
    " {\n",
    "    'DatasetArgs' :{\n",
    "        'Dataset' : 'LongVOS',\n",
    "        'Set' : 'test',\n",
    "    },\n",
    "    'TrackingAnythingArgs' : {\n",
    "            'use_refinement' : False\n",
    "                }\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq names: ['TVfoBiU2', 'R3UJGYwy', '81Oju1e7', 'JefVS4hq', 'FmzvB44A', 'shW04XAm', 'PhiOlFlA', 'w3XiaDdm', 'gUca15ef', 'gw84JOqH', 'bDv75KJl', 'irsQQbUO', 'B9fFIrgq', 'YMRntd88', 'T4pEUdxQ', 'UtEnRpiP', 'RCgebO9V', 'ealukzgh', 'LWVlKBlK', 'UBITNvZb', 'qeD7QcDW', 'Oj1z5fO0', 'Y6tOZtQV', 'UykV25jo', '6mNj04tC', 'GmfSNNa3', 'AIv9mv7T', 'aSNxf5ms', '58jpr19X', 'Kz1wgslb', '058oeZ2p', 'vlKROEmy', '5NImTLT6', 'T9woxlyM', '8kv99Cop', 'yxCvN6OJ', 'xL5OHe3Y', 'yfzVCnvU', 'HQ2UJdrW', 'Gz3GEPPC', 'tAyQIobn', 'yWnanBID', '7im4LEcb', '711gAS21', 'YHUoFe2u', '5VwnMLaz', 'MI0uwoMt', 'R4xwHrdP', 'GchnEETZ', 'PQPOuFNO']\n",
      "Running Test: Refined_HQ_mask_bbox_pos_neg_C&Poly_LongVOS_ZhkQI\n",
      "Initializing BaseSegmenter to cuda:0\n",
      "<All keys matched successfully>\n",
      "Hyperparameters read from the model weights: C^k=64, C^v=512, C^h=64\n",
      "Single object mode: False\n",
      "Sam Refinement ACTIVATED. Mode: mask_bbox_pos_neg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image: 100%|██████████| 498/498 [02:51<00:00,  2.90it/s]\n",
      "Tracking image: 100%|██████████| 583/583 [01:57<00:00,  4.94it/s]\n",
      "Tracking image: 100%|██████████| 553/553 [03:05<00:00,  2.98it/s]\n",
      "Tracking image: 100%|██████████| 315/315 [01:23<00:00,  3.79it/s]\n",
      "Tracking image: 100%|██████████| 402/402 [00:50<00:00,  7.93it/s]\n",
      "Tracking image: 100%|██████████| 480/480 [01:00<00:00,  7.96it/s]\n",
      "Tracking image: 100%|██████████| 975/975 [05:34<00:00,  2.92it/s]\n",
      "Tracking image: 100%|██████████| 1965/1965 [47:15<00:00,  1.44s/it]\n",
      "Tracking image: 100%|██████████| 422/422 [19:22<00:00,  2.76s/it]\n",
      "Tracking image: 100%|██████████| 261/261 [00:33<00:00,  7.85it/s]\n",
      "Tracking image: 100%|██████████| 263/263 [01:27<00:00,  2.99it/s]\n",
      "Tracking image: 100%|██████████| 526/526 [02:59<00:00,  2.92it/s]\n",
      "Tracking image: 100%|██████████| 352/352 [00:29<00:00, 11.99it/s]\n",
      "Tracking image: 100%|██████████| 272/272 [01:34<00:00,  2.88it/s]\n",
      "Tracking image: 100%|██████████| 321/321 [00:57<00:00,  5.62it/s]\n",
      "Tracking image: 100%|██████████| 431/431 [02:28<00:00,  2.90it/s]\n",
      "Tracking image: 100%|██████████| 444/444 [00:36<00:00, 12.15it/s]\n",
      "Tracking image: 100%|██████████| 360/360 [02:02<00:00,  2.94it/s]\n",
      "Tracking image: 100%|██████████| 498/498 [02:45<00:00,  3.02it/s]\n",
      "Tracking image: 100%|██████████| 574/574 [03:14<00:00,  2.95it/s]\n",
      "Tracking image: 100%|██████████| 696/696 [04:00<00:00,  2.89it/s]\n",
      "Tracking image: 100%|██████████| 257/257 [01:21<00:00,  3.17it/s]\n",
      "Tracking image: 100%|██████████| 338/338 [01:54<00:00,  2.96it/s]\n",
      "Tracking image: 100%|██████████| 601/601 [03:22<00:00,  2.97it/s]\n",
      "Tracking image: 100%|██████████| 450/450 [01:14<00:00,  6.04it/s]\n",
      "Tracking image: 100%|██████████| 574/574 [03:16<00:00,  2.92it/s]\n",
      "Tracking image: 100%|██████████| 276/276 [01:34<00:00,  2.91it/s]\n",
      "Tracking image: 100%|██████████| 470/470 [02:41<00:00,  2.91it/s]\n",
      "Tracking image: 100%|██████████| 1505/1505 [03:02<00:00,  8.26it/s]\n",
      "Tracking image: 100%|██████████| 521/521 [00:39<00:00, 13.31it/s]\n",
      "Tracking image: 100%|██████████| 837/837 [01:00<00:00, 13.86it/s]\n",
      "Tracking image: 100%|██████████| 344/344 [01:57<00:00,  2.93it/s]\n",
      "Tracking image: 100%|██████████| 452/452 [00:43<00:00, 10.45it/s]\n",
      "Tracking image: 100%|██████████| 551/551 [00:36<00:00, 14.89it/s]\n",
      "Tracking image: 100%|██████████| 685/685 [03:51<00:00,  2.96it/s]\n",
      "Tracking image: 100%|██████████| 500/500 [02:32<00:00,  3.29it/s]\n",
      "Tracking image: 100%|██████████| 594/594 [02:31<00:00,  3.92it/s]\n",
      "Tracking image: 100%|██████████| 538/538 [01:21<00:00,  6.58it/s]\n",
      "Tracking image: 100%|██████████| 2000/2000 [10:45<00:00,  3.10it/s]\n",
      "Tracking image: 100%|██████████| 377/377 [17:39<00:00,  2.81s/it]\n",
      "Tracking image: 100%|██████████| 277/277 [01:40<00:00,  2.75it/s]\n",
      "Tracking image: 100%|██████████| 320/320 [01:26<00:00,  3.72it/s]\n",
      "Tracking image: 100%|██████████| 693/693 [03:58<00:00,  2.90it/s]\n",
      "Tracking image: 100%|██████████| 691/691 [03:57<00:00,  2.91it/s]\n",
      "Tracking image: 100%|██████████| 345/345 [01:52<00:00,  3.07it/s]\n",
      "Tracking image: 100%|██████████| 310/310 [01:38<00:00,  3.14it/s]\n",
      "Tracking image: 100%|██████████| 1279/1279 [05:49<00:00,  3.65it/s]\n",
      "Tracking image: 100%|██████████| 368/368 [00:26<00:00, 13.99it/s]\n",
      "Tracking image: 100%|██████████| 1365/1365 [05:03<00:00,  4.50it/s]\n",
      "Tracking image: 100%|██████████| 1185/1185 [22:15<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: XMEM_HQ_LongVOS_CZoob\n",
      "Initializing BaseSegmenter to cuda:0\n",
      "<All keys matched successfully>\n",
      "Hyperparameters read from the model weights: C^k=64, C^v=512, C^h=64\n",
      "Single object mode: False\n",
      "Sam Refinement NOT ACTIVATED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking image: 100%|██████████| 498/498 [00:43<00:00, 11.49it/s]\n",
      "Tracking image: 100%|██████████| 583/583 [03:12<00:00,  3.03it/s]\n",
      "Tracking image: 100%|██████████| 553/553 [03:11<00:00,  2.89it/s]\n",
      "Tracking image: 100%|██████████| 315/315 [00:37<00:00,  8.39it/s]\n",
      "Tracking image: 100%|██████████| 402/402 [01:15<00:00,  5.34it/s]\n",
      "Tracking image: 100%|██████████| 480/480 [00:45<00:00, 10.48it/s]\n",
      "Tracking image: 100%|██████████| 975/975 [05:12<00:00,  3.12it/s]\n",
      "Tracking image: 100%|██████████| 1965/1965 [1:04:11<00:00,  1.96s/it]\n",
      "Tracking image: 100%|██████████| 422/422 [14:15<00:00,  2.03s/it]\n",
      "Tracking image: 100%|██████████| 261/261 [01:52<00:00,  2.32it/s]\n",
      "Tracking image: 100%|██████████| 263/263 [01:22<00:00,  3.17it/s]\n",
      "Tracking image: 100%|██████████| 526/526 [03:03<00:00,  2.86it/s]\n",
      "Tracking image: 100%|██████████| 352/352 [01:13<00:00,  4.81it/s]\n",
      "Tracking image: 100%|██████████| 272/272 [01:24<00:00,  3.22it/s]\n",
      "Tracking image: 100%|██████████| 321/321 [01:15<00:00,  4.25it/s]\n",
      "Tracking image: 100%|██████████| 431/431 [02:12<00:00,  3.24it/s]\n",
      "Tracking image: 100%|██████████| 444/444 [01:40<00:00,  4.42it/s]\n",
      "Tracking image: 100%|██████████| 360/360 [01:33<00:00,  3.86it/s]\n",
      "Tracking image: 100%|██████████| 498/498 [01:33<00:00,  5.35it/s]\n",
      "Tracking image: 100%|██████████| 574/574 [02:17<00:00,  4.18it/s]\n",
      "Tracking image: 100%|██████████| 696/696 [03:11<00:00,  3.64it/s]\n",
      "Tracking image: 100%|██████████| 257/257 [00:58<00:00,  4.39it/s]\n",
      "Tracking image: 100%|██████████| 338/338 [01:09<00:00,  4.83it/s]\n",
      "Tracking image: 100%|██████████| 601/601 [01:23<00:00,  7.22it/s]\n",
      "Tracking image: 100%|██████████| 450/450 [01:36<00:00,  4.68it/s]\n",
      "Tracking image: 100%|██████████| 574/574 [01:25<00:00,  6.73it/s]\n",
      "Tracking image: 100%|██████████| 276/276 [00:32<00:00,  8.44it/s]\n",
      "Tracking image: 100%|██████████| 470/470 [01:26<00:00,  5.42it/s]\n",
      "Tracking image: 100%|██████████| 1505/1505 [20:36<00:00,  1.22it/s]\n",
      "Tracking image: 100%|██████████| 521/521 [15:44<00:00,  1.81s/it]\n",
      "Tracking image: 100%|██████████| 837/837 [06:40<00:00,  2.09it/s]\n",
      "Tracking image: 100%|██████████| 344/344 [02:13<00:00,  2.58it/s]\n",
      "Tracking image: 100%|██████████| 452/452 [02:22<00:00,  3.16it/s]\n",
      "Tracking image: 100%|██████████| 551/551 [01:35<00:00,  5.77it/s]\n",
      "Tracking image: 100%|██████████| 685/685 [03:22<00:00,  3.38it/s]\n",
      "Tracking image: 100%|██████████| 500/500 [02:30<00:00,  3.33it/s]\n",
      "Tracking image: 100%|██████████| 594/594 [02:51<00:00,  3.46it/s]\n",
      "Tracking image: 100%|██████████| 538/538 [02:35<00:00,  3.47it/s]\n",
      "Tracking image: 100%|██████████| 2000/2000 [38:27<00:00,  1.15s/it]\n",
      "Tracking image: 100%|██████████| 377/377 [10:46<00:00,  1.72s/it]\n",
      "Tracking image: 100%|██████████| 277/277 [01:08<00:00,  4.02it/s]\n",
      "Tracking image: 100%|██████████| 320/320 [05:44<00:00,  1.08s/it]\n",
      "Tracking image: 100%|██████████| 693/693 [04:24<00:00,  2.62it/s]\n",
      "Tracking image: 100%|██████████| 691/691 [02:14<00:00,  5.13it/s]\n",
      "Tracking image: 100%|██████████| 345/345 [04:31<00:00,  1.27it/s]\n",
      "Tracking image: 100%|██████████| 310/310 [01:55<00:00,  2.69it/s]\n",
      "Tracking image: 100%|██████████| 1279/1279 [10:26<00:00,  2.04it/s]\n",
      "Tracking image: 100%|██████████| 368/368 [14:18<00:00,  2.33s/it]\n",
      "Tracking image: 100%|██████████| 1365/1365 [15:28<00:00,  1.47it/s]\n",
      "Tracking image: 100%|██████████| 1185/1185 [55:35<00:00,  2.82s/it] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "VideoLoader = LongVideoDataset(root_folder=longvos_root, filelist= f\"{runtimeargs_lst[0]['DatasetArgs']['Set']}.txt\")\n",
    "for runtimeargs in runtimeargs_lst:\n",
    "    runname = '{}{}{}{}_{}_{}'.format(\n",
    "        'Refined' if runtimeargs['TrackingAnythingArgs']['use_refinement']  else 'XMEM',\n",
    "        '_HQ' if 'HQ' in sys.prefix.split('.')[-1] else '_SAM',\n",
    "        '_'+ runtimeargs['TrackingAnythingArgs']['refinement_mode'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] else '',\n",
    "        '_' + runtimeargs['TrackingAnythingArgs']['addArgs1'] if runtimeargs['TrackingAnythingArgs']['use_refinement'] and runtimeargs['TrackingAnythingArgs']['addArgs1'] != '' else '',\n",
    "        'Davis_{}_{}_{}'.format(runtimeargs['DatasetArgs']['Year'],runtimeargs['DatasetArgs']['Set'],runtimeargs['DatasetArgs']['Resolution']) if runtimeargs['DatasetArgs']['Dataset'] == 'Davis' else runtimeargs['DatasetArgs']['Dataset'],\n",
    "        ''.join(random.choice(string.ascii_letters) for _ in range(5))\n",
    "    )\n",
    "    print(f'Running Test: {runname}')\n",
    "    model = TrackingAnything(SAM_checkpoint, xmem_checkpoint, None,runtimeargs['TrackingAnythingArgs'], save_inner_masks_folder = runname)\n",
    "    masks, logits, painted_images = run_model_on_longVOS_set(name = runname,model = model,videoLoader = VideoLoader,compute_metrics = False, save_masks=True, compute_video=True,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(longvos_root,'test_meta.json'), 'r') as archivo:\n",
    "    datos = json.load(archivo)\n",
    "all_videos = []\n",
    "for video in datos['videos']:\n",
    "    frame_data  = datos['videos'][video]['objects']['1']\n",
    "    start = int(frame_data['frame_range']['start'])\n",
    "    end = int(frame_data['frame_range']['end'])\n",
    "    video_file_names = []\n",
    "    for i in range(start, end + 1, 5):\n",
    "        file_name = f\"{video}/{str(i).zfill(8)}.jpg\"\n",
    "        video_file_names.append(file_name)\n",
    "    all_videos.append(video_file_names)\n",
    "\n",
    "longvos_images = load_images_from_folder(os.path.join(longvos_root,'JPEGImages'),all_videos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq names: ['TVfoBiU2', 'R3UJGYwy', '81Oju1e7', 'JefVS4hq', 'FmzvB44A', 'shW04XAm', 'PhiOlFlA', 'w3XiaDdm', 'gUca15ef', 'gw84JOqH', 'bDv75KJl', 'irsQQbUO', 'B9fFIrgq', 'YMRntd88', 'T4pEUdxQ', 'UtEnRpiP', 'RCgebO9V', 'ealukzgh', 'LWVlKBlK', 'UBITNvZb', 'qeD7QcDW', 'Oj1z5fO0', 'Y6tOZtQV', 'UykV25jo', '6mNj04tC', 'GmfSNNa3', 'AIv9mv7T', 'aSNxf5ms', '58jpr19X', 'Kz1wgslb', '058oeZ2p', 'vlKROEmy', '5NImTLT6', 'T9woxlyM', '8kv99Cop', 'yxCvN6OJ', 'xL5OHe3Y', 'yfzVCnvU', 'HQ2UJdrW', 'Gz3GEPPC', 'tAyQIobn', 'yWnanBID', '7im4LEcb', '711gAS21', 'YHUoFe2u', '5VwnMLaz', 'MI0uwoMt', 'R4xwHrdP', 'GchnEETZ', 'PQPOuFNO']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gt_masks, _, all_masks_id = videoLoader.get_all_masks('TVfoBiU2', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "def print_curve(precision, recall,column):\n",
    "    figure(figsize=(12, 7), dpi=80)\n",
    "    interpolated_precisions = np.maximum.accumulate(precision[::-1])[::-1]\n",
    "    plt.plot(recall, interpolated_precisions, color='red', linestyle='-')\n",
    "    plt.plot(recall, precision, color='blue', linestyle='-')\n",
    "    # Add legend, grid, title, etc.\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(f'Precision Recall Curve', fontsize=20)\n",
    "    plt.xlabel('Recall', fontsize=14)\n",
    "    plt.ylabel('Precision', fontsize=14)\n",
    " \n",
    "    plt.grid(False)\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
